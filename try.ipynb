{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24351743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d2f69b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1401926350.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 27\u001b[0;36m\u001b[0m\n\u001b[0;31m    Tax-5%,\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# invoiceNumber\tInvoiceDate\tDueDate\tProduct line\tUnit price\tQuantity\tTax 5%\tTotal\tDate\tTime\tPayment\tTELEFON\n",
    "categories = [\n",
    "    \"invoiceNumber\",\n",
    "    \"InvoiceDate\",\n",
    "    \"DueDate\",\n",
    "    \"Product line\",\n",
    "    \"Unit price\",\n",
    "    \"Tax 5%\",\n",
    "    \"Total\",\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Payment\",\n",
    "    \"TELEFONE\"\n",
    "]\n",
    "\n",
    "news_group_data = text(\n",
    "    subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    dict(\n",
    "        invoiceNumber,\n",
    "        InvoiceDate,\n",
    "        DueDate,\n",
    "        Productline,\n",
    "        Unitprice,\n",
    "        Tax-5%,\n",
    "        Total,\n",
    "        Date,\n",
    "        Time,\n",
    "        Payment,\n",
    "        TELEFONE\n",
    "        text=news_group_data[\"data\"],\n",
    "        target=news_group_data[\"target\"]\n",
    "    )\n",
    ")\n",
    "df[\"target\"] = df.target.map(lambda x: categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5682dad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for /home/sunil/.keras-ocr/craft_mlt_25k.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 11:35:03.048483: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-08 11:35:03.050251: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-08 11:35:03.051577: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-08 11:35:03.310090: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-05-08 11:35:03.375982: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-08 11:35:03.377435: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-08 11:35:03.378761: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-08 11:35:03.646099: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-08 11:35:03.648132: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-08 11:35:03.649608: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-08 11:35:03.858464: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-05-08 11:35:03.919855: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-08 11:35:03.921275: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-08 11:35:03.922532: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-08 11:35:04.070956: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'lambda_7/scan/while/Fill/lambda_7/stack_1' with dtype int32 and shape [1]\n",
      "\t [[{{node lambda_7/scan/while/Fill/lambda_7/stack_1}}]]\n",
      "2023-05-08 11:35:04.071088: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'lambda_7/scan/while/Fill/lambda_7/stack_1' with dtype int32 and shape [1]\n",
      "\t [[{{node lambda_7/scan/while/Fill/lambda_7/stack_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for /home/sunil/.keras-ocr/crnn_kurapan.h5\n"
     ]
    }
   ],
   "source": [
    "import keras_ocr\n",
    "\n",
    "pipeline = keras_ocr.pipeline.Pipeline()\n",
    "image = keras_ocr.tools.read('/home/sunil/Desktop/np/data_local/jot.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675f9f8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 11:35:21.755864: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 758644736 exceeds 10% of free system memory.\n",
      "2023-05-08 11:35:22.513500: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 758644736 exceeds 10% of free system memory.\n",
      "2023-05-08 11:35:22.869394: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 758644736 exceeds 10% of free system memory.\n",
      "2023-05-08 11:35:23.854260: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 758644736 exceeds 10% of free system memory.\n",
      "2023-05-08 11:35:24.167507: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 189530112 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 11:35:33.828882: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-08 11:35:33.831247: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-08 11:35:33.832905: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-08 11:35:34.080286: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-05-08 11:35:34.157073: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-08 11:35:34.158894: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-08 11:35:34.160470: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-08 11:35:34.478305: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-08 11:35:34.480304: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-08 11:35:34.482053: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-08 11:35:34.730542: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-05-08 11:35:34.807300: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-08 11:35:34.809269: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-08 11:35:34.810834: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 7s 1s/step\n"
     ]
    }
   ],
   "source": [
    "prediction_groups = pipeline.recognize([image])\n",
    "texts = [word[0] for prediction in prediction_groups for word in prediction]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019ee894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at text_classification_model.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_classification_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m classes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minvoiceNumber\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvoiceDate\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDueDate\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct line\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnit price\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuantity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTax 5\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPayment\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTELEFONE\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# replace with your own class labels\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# preprocess the text\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/saving/saving_api.py:212\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    205\u001b[0m         filepath,\n\u001b[1;32m    206\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    207\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    208\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/saving/legacy/save.py:230\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    231\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m         )\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    236\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    237\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at text_classification_model.h5"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "model = tf.keras.models.load_model('text_classification_model.h5')\n",
    "classes = ['invoiceNumber','InvoiceDate','DueDate','Product line','Unit price','Quantity','Tax 5%','Total','Date','Time','Payment','TELEFONE']  # replace with your own class labels\n",
    "\n",
    "for text in texts:\n",
    "    # preprocess the text\n",
    "    processed_text = preprocess(text)\n",
    "\n",
    "    # predict the class\n",
    "    prediction = model.predict(processed_text)\n",
    "    predicted_class = classes[prediction.argmax()]\n",
    "    \n",
    "    print(f'Text: {text} - Predicted class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2661d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from langdetect import detect\n",
    "import cv2\n",
    "from wand.image import Image as wi   \n",
    "from pdf2image import convert_from_path\n",
    "import io      \n",
    "import matplotlib.pyplot as plt\n",
    "# import keras_ocr\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74dbb89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######PRECISION LOGISTICS - STATEMENT OF ACCOUNT - NOV 1,2022.pdf#######\n",
      "#######PORT-INV-00015798.pdf#######\n",
      "#######PORT-INV-00016250.pdf#######\n",
      "#######PORT-INV-00016248.pdf#######\n",
      "#######PGAS-INV-0215488.pdf#######\n",
      "#######PORT-INV-00016327.pdf#######\n",
      "#######MARKSMAN Maintenance.pdf#######\n",
      "#######MAIN-INV-00331962.pdf#######\n",
      "#######PGAS-INV-0184276.pdf#######\n",
      "#######PRECISION LOGISTICS MONTHLY INVOICE - NOV 2022.pdf#######\n",
      "#######PORT-INV-00016292.pdf#######\n",
      "#######PORT-INV-00016726.pdf#######\n",
      "#######page_3.jpg#######\n",
      "APR 2021\n",
      "MAY 2021\n",
      "JUN 2021\n",
      "JUN 2021\n",
      "JUN 2021\n",
      "JUN 2021\n",
      "JUN 2021\n",
      "JUN 2021\n",
      "JUN 2021\n",
      "JuL2021\n",
      "\n",
      "JuL2021\n",
      "\n",
      "JuL2021\n",
      "\n",
      "SEP 2021\n",
      "\n",
      "SEP 2021\n",
      "\n",
      "NOV 2021\n",
      "NOV 2021\n",
      "NOV 2021\n",
      "NOV 2021\n",
      "NOV 2021\n",
      "NOV 2021\n",
      "NOV 2021\n",
      "NOV 2021\n",
      "NOV 2021\n",
      "DEC 2021\n",
      "JAN 2022\n",
      "NOV 22\n",
      "\n",
      "BUD260A - CL3406 - DAMAGES\n",
      "BUD299 - CM7647 - DAMAGES\n",
      "BUD307 - CL3402 - DAMAGES\n",
      "BUD308 - CM7646 - DAMAGES\n",
      "BUD309 - CL3447 - DAMAGES\n",
      "BUD310 - CK7763 - DAMAGES\n",
      "BUD311 - CL1728 - DAMAGES\n",
      "BUD312 - CL3406 - DAMAGES\n",
      "BUD316 - CM7647 - DAMAGES\n",
      "BUD319 - 4663HY - DAMAGES\n",
      "BUD320 - 7748HX - DAMAGES\n",
      "BUD321 - CL1018 - DAMAGES\n",
      "BUD327 - CL1010 - DAMAGES\n",
      "BUD328 - 6148HG - DAMAGES\n",
      "BUD329 - CL3408 - DAMAGES\n",
      "BUD330 - - DAMAGES\n",
      "BUD331 - 7748HX - DAMAGES\n",
      "BUD332 - CL3404 - DAMAGES\n",
      "BUD333 - CL1729 - DAMAGES\n",
      "BUD334 - CK5577 - DAMAGES\n",
      "BUD335 - CM0740 - DAMAGES\n",
      "BUD336 - CL3410 - DAMAGES\n",
      "BUD337 - CL3401 - DAMAGES\n",
      "WIRE\n",
      "BUD283 - CL5898 - MCR 68243\n",
      "MONTHLY RENTAL - NOV 22\n",
      "\n",
      "TOTALS\n",
      "\n",
      "568.29\n",
      "4,840.24\n",
      "4,077.35\n",
      "4,172.48\n",
      "4,031.83\n",
      "3,754.41\n",
      "3,620.76\n",
      "4,027.41\n",
      "4,309.44\n",
      "\n",
      "82.36\n",
      "82.36\n",
      "\n",
      "101.08\n",
      "\n",
      "274.21\n",
      "\n",
      "727.50\n",
      "\n",
      "77.71\n",
      "\n",
      "105.20\n",
      "560.64\n",
      "287.36\n",
      "268.89\n",
      "373.25\n",
      "7,780.04\n",
      "28.11\n",
      "\n",
      "37,420.00\n",
      "\n",
      "276,964.39\n",
      "\n",
      "-1,350.00\n",
      "\n",
      "85.24\n",
      "726.04\n",
      "611.60\n",
      "625.87\n",
      "604.77\n",
      "563.16\n",
      "543.11\n",
      "604.11\n",
      "646.42\n",
      "\n",
      "1235\n",
      "\n",
      "1235\n",
      "\n",
      "15.16\n",
      "\n",
      "41.13\n",
      "109.12\n",
      "\n",
      "11.66\n",
      "\n",
      "0.00\n",
      "\n",
      "15.78\n",
      "\n",
      "84.10\n",
      "\n",
      "43.10\n",
      "\n",
      "40.33\n",
      "\n",
      "55.99\n",
      "\n",
      "1,167.01\n",
      "4.22\n",
      "\n",
      "5,613.00\n",
      "\n",
      "40,606.59\n",
      "\n",
      "653.53\n",
      "5,566.27\n",
      "4,688.96\n",
      "4,798.35\n",
      "4,636.61\n",
      "4,317.57\n",
      "4,163.87\n",
      "4,631.52\n",
      "4,955.86\n",
      "\n",
      "94.71\n",
      "94.71\n",
      "\n",
      "116.24\n",
      "\n",
      "315.35\n",
      "\n",
      "836.62\n",
      "\n",
      "89.37\n",
      "0.00\n",
      "\n",
      "120.98\n",
      "\n",
      "644.74\n",
      "\n",
      "330.47\n",
      "\n",
      "309.22\n",
      "\n",
      "429.24\n",
      "8,947.05\n",
      "\n",
      "32.33\n",
      "\n",
      "43,033.00\n",
      "\n",
      "316,220.99\n",
      "\n",
      "95,795.00\n",
      "13,455.00\n",
      "700,306.96\n",
      "716,645.62\n",
      "692,488.43\n",
      "644,838.86\n",
      "621,883.73\n",
      "691,728.32\n",
      "740,169.38\n",
      "14,145.00\n",
      "14,145.00\n",
      "17,360.45\n",
      "47,961.46\n",
      "127,242.73\n",
      "13,592.00\n",
      "0.00\n",
      "18,400.00\n",
      "98,059.40\n",
      "50,261.46\n",
      "47,029.53\n",
      "65,282.97\n",
      "1,360,765.79\n",
      "4,917.40\n",
      "-1,037,712.14\n",
      "-316,084.99\n",
      "6,632,887.15\n",
      "\n",
      "21,657,067.86\n",
      "\n",
      "9,677,298.37\n",
      "\n",
      "9,690,753.37\n",
      "\n",
      "10,391,060.32\n",
      "11,107,705.94\n",
      "11,800,194.36\n",
      "12,445,033.22\n",
      "13,066,916.95\n",
      "13,758,645.27\n",
      "14,498,814.66\n",
      "14,512,959.66\n",
      "14,527,104.66\n",
      "14,544,465.10\n",
      "14,592,426.56\n",
      "14,719,669.29\n",
      "14,733,261.29\n",
      "14,733,261.29\n",
      "14,751,661.29\n",
      "14,849,720.69\n",
      "14,899,982.15\n",
      "14,947,011.68\n",
      "15,012,294.65\n",
      "16,373,060.44\n",
      "16,377,977.84\n",
      "15,340,265.70\n",
      "15,024,180.71\n",
      "21,657,067.86\n",
      "\n",
      "#######PORT-INV-00015890.pdf#######\n",
      "#######PORT-INV-00016227.pdf#######\n",
      "#######Precision Venessa Reynolds Inv 43876.pdf#######\n",
      "#######PGAS-INV-0194111.pdf#######\n",
      "#######Invoice-205257.pdf#######\n",
      "#######ORKIN MGMT FEE -NOV'22.pdf#######\n",
      "#######MARKSMAN PLANT PURCHASE.pdf#######\n",
      "#######PRECISION RENTAL & MNTCE OF PREMISES -NOV'2022.pdf#######\n",
      "#######PORT-INV-00016320.pdf#######\n",
      "#######page_1.jpg#######\n",
      "***HISTORICAL*** Invoice PORT-INV-00016320\n",
      "\n",
      "IGL Limited [Date  To4/032020 |\n",
      "593-595 Spanish Town Road\n",
      "\n",
      "P.O. Box 224\n",
      "Kingston 11 876\n",
      "\n",
      "Bill To: Ship To:\n",
      "\n",
      "GUARDSMAN GROUP LIMITED GUARDSMAN GROUP LIMITED\n",
      "5 & 6 CARVALHO DRIVE 5 & 6 CARVALHO DRIVE\n",
      "KINGSTON 10 KINGSTON 10\n",
      "\n",
      "Purchase Order No. Customer ID Salesperson ID Shipping Method Payment Terms Req Ship Date | Master No.\n",
      "PG0088-000 AREA2-CAT-PM DELIVERIES 15 DAYS 04/03/2020 1,689,987\n",
      "Ordered B/O Item Number Description Discount Unit Price | Ext. Price\n",
      "1.00 i 0.00 [ 999-01-0014 LPG SUPERGAS 0.00 9,305.00 9,305.00\n",
      "\n",
      "1.00 0.00 | 801-01-0001 SEAL 100# BLUE TAMPER PROOF -45KG CY|| 0.00 10.00 10.00\n",
      "\n",
      "IGL Limited Subtotal\n",
      "Misc\n",
      "\n",
      "JA$9,315.00\n",
      "JA$0.00\n",
      "JA$1.50\n",
      "\n",
      "JA$0.00\n",
      "\n",
      "CERTIFIED COPY OF THE ORIGINAL\n",
      "\n",
      "Freight\n",
      "Trade Discount\n",
      "\n",
      "Dealer:Norris Phinn\n",
      "Serial #:018323\n",
      "\n",
      "JA$0.00\n",
      "\n",
      "JA$9,316.50\n",
      "\n",
      "Tracy-Ann Spencer-Plummer\n",
      "\n",
      "#######page_2.jpg#######\n",
      "\n",
      "#######MAIN-INV-00336216.pdf#######\n",
      "#######Precision JPS PO 625.pdf#######\n",
      "#######Invoice-205235.pdf#######\n"
     ]
    }
   ],
   "source": [
    "indir = \"/home/sunil/Desktop/np/flagged/input_image/\"\n",
    "for root, dirs, filenames in os.walk(indir):\n",
    "    for filename in filenames:\n",
    "        print('#######' + filename + '#######')\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "#             gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#             thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "            im = Image.open(os.path.join(indir, filename))\n",
    "            text = pytesseract.image_to_string(im, lang='eng')\n",
    "            print(text)\n",
    "        elif filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(indir, filename)\n",
    "            images = convert_from_path(pdf_path)\n",
    "            for i, image in enumerate(images):\n",
    "                image.save(f'/home/sunil/Desktop/np/flagged/input_image/page_{i+1}.jpg', 'JPEG')\n",
    "        else:\n",
    "            print(\"Input the correct file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8126ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Restaurant Receipt\n",
    "\n",
    "Name Jane Smith 002\n",
    "\n",
    "Receipt #\n",
    "\n",
    "Email jasmith@noemail.com  Date 4/8/2019\n",
    "\n",
    "Phone Number (123) 123-4567 Time 5:00 PM\n",
    "\n",
    "Ordered Items\n",
    "\n",
    "Food Name Quantity Price Total\n",
    "\n",
    "1 Food A 5 5 25\n",
    "\n",
    "2 Food B 5 5 25\n",
    "\n",
    "3 Food C 5 5 25\n",
    "\n",
    "4 Drink D 5 2 10\n",
    "\n",
    "5 Drink E 5 2 10\n",
    "Subtotal $95\n",
    "Tax % 5\n",
    "\n",
    "I\n",
    "\n",
    "Total $99.75\n",
    "\n",
    "Hope you enjoy your dining experience!\n",
    "\n",
    "1929 Brown Bear Drive, Elsinore,CA, 92330\n",
    "Phone: (123) 1234567 - nfo@xyzrestaurant com - st xyzrestaurant com\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d63bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "maxlen = 100  # maximum length of input sequence\n",
    "num_words = 10000  # maximum number of words in the tokenizer's vocabulary\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "X = tokenizer.texts_to_sequences(text)\n",
    "X = pad_sequences(X, maxlen=maxlen, padding='post')\n",
    "\n",
    "y = ['invoiceNumber','InvoiceDate','DueDate','Product line','Unit price','Quantity','Tax 5%','Total','Date','Time','Payment','TELEFONE']  # replace with your own label data\n",
    "\n",
    "# split the dataset into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d6fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "338060be",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3049929523.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    <wordcloud.wordcloud.WordCloud at 0x7faf5a245ba8>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<wordcloud.wordcloud.WordCloud at 0x7faf5a245ba8>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7623b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement prodigy (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for prodigy\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install prodigy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"model-best\")  # load the spacy model\n",
    "\n",
    "def process_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    return pytesseract.image_to_string(thresh, lang='eng')\n",
    "\n",
    "def process_file(file_path):\n",
    "    text = \"\"\n",
    "    if file_path.endswith(\".jpg\") or file_path.endswith(\".jpeg\") or file_path.endswith(\".png\"):\n",
    "        im = cv2.imread(file_path)\n",
    "        text = process_image(im)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        images = convert_from_path(file_path)\n",
    "        for i, image in enumerate(images):\n",
    "            image.save(f'special/page_{i+1}.jpg', 'JPEG')\n",
    "            text += process_image(image)\n",
    "    else:\n",
    "        print(\"Input the correct file\")\n",
    "    return text\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    for root, dirs, filenames in os.walk(directory_path):\n",
    "        for filename in filenames:\n",
    "#             print('#######' + filename + '#######')\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            text = process_file(file_path)\n",
    "            doc = nlp(text)\n",
    "            for ent in doc.ents:\n",
    "                print(ent.text, ent.label_,)\n",
    "\n",
    "process_directory(\"/home/sunil/Desktop/np/uploads/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ce314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "import spacy\n",
    "import shutil\n",
    "\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\")  # load the spacy model\n",
    "\n",
    "def process_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    return pytesseract.image_to_string(thresh, lang='eng')\n",
    "\n",
    "def process_file(file_path):\n",
    "    text = \"\"\n",
    "    if file_path.endswith(\".jpg\") or file_path.endswith(\".jpeg\") or file_path.endswith(\".png\"):\n",
    "        im = cv2.imread(file_path)\n",
    "        text = process_image(im)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        images = convert_from_path(file_path)\n",
    "        for i, image in enumerate(images):\n",
    "            image.save(f'special/page_{i+1}.jpg', 'JPEG')\n",
    "            text += process_image(image)\n",
    "    else:\n",
    "        print(\"Input the correct file\")\n",
    "    return text\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    for root, dirs, filenames in os.walk(input_dir):\n",
    "        for filename in filenames:\n",
    "            print('#######' + filename + '#######')\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            text = process_file(input_path)\n",
    "            doc = nlp(text)\n",
    "            for ent in doc.ents:\n",
    "                print(ent.text, ent.label_)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            shutil.move(input_path, output_path)\n",
    "\n",
    "process_directory(\"/home/sunil/Desktop/np/uploads\", \"/home/sunil/Desktop/np/special\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae57006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import pytesseract\n",
    "import spacy\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02ae5205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######63a085d8d4fdce0328d9226c_Screen Shot 2022-12-19 at 10.38.35 AM.png#######\n",
      "Processed text:\n",
      "{\n",
      " \"DATE\": \"Jan 132023\",\n",
      " \"Items\": []\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"DATE\": \"Jan 132023\",\n",
      " \"Items\": []\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"DATE\": \"Jan 132023\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"John Hopkins\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"DATE\": \"Jan 132023\",\n",
      " \"EMAIL\": \"Jhopkins@thisiswork.com\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"John Hopkins\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"DATE\": \"Jan 132023\",\n",
      " \"DATE-1\": \"Jan 16 2023, 13:13\",\n",
      " \"EMAIL\": \"Jhopkins@thisiswork.com\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"John Hopkins\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"DATE\": \"Jan 132023\",\n",
      " \"DATE-1\": \"Jan 16 2023, 13:13\",\n",
      " \"DATE-2\": \"Jan 16 2023, 13:13\\n\\nTHIRD PARTY BILL\",\n",
      " \"EMAIL\": \"Jhopkins@thisiswork.com\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"John Hopkins\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"COUNTRY\": \"United States\",\n",
      " \"DATE\": \"Jan 132023\",\n",
      " \"DATE-1\": \"Jan 16 2023, 13:13\",\n",
      " \"DATE-2\": \"Jan 16 2023, 13:13\\n\\nTHIRD PARTY BILL\",\n",
      " \"EMAIL\": \"Jhopkins@thisiswork.com\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"John Hopkins\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"COUNTRY\": \"United States\",\n",
      " \"COUNTRY-3\": \"United States\",\n",
      " \"DATE\": \"Jan 132023\",\n",
      " \"DATE-1\": \"Jan 16 2023, 13:13\",\n",
      " \"DATE-2\": \"Jan 16 2023, 13:13\\n\\nTHIRD PARTY BILL\",\n",
      " \"EMAIL\": \"Jhopkins@thisiswork.com\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"John Hopkins\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"AMOUNT\": 5245676.0,\n",
      " \"COUNTRY\": \"United States\",\n",
      " \"COUNTRY-3\": \"United States\",\n",
      " \"DATE\": \"Jan 132023\",\n",
      " \"DATE-1\": \"Jan 16 2023, 13:13\",\n",
      " \"DATE-2\": \"Jan 16 2023, 13:13\\n\\nTHIRD PARTY BILL\",\n",
      " \"EMAIL\": \"Jhopkins@thisiswork.com\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"John Hopkins\"\n",
      "}\n",
      "<class 'dict'>\n",
      "Moved file to output.html\n",
      "#######tax-invoice-services-local.jpg#######\n",
      "Processed text:\n",
      "{\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"PUSA ROAD,NEW DELHI-110005\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"PUSA ROAD,NEW DELHI-110005\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\",\n",
      " \"NAME-1\": \"RK Electrical Works\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"PUSA ROAD,NEW DELHI-110005\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\",\n",
      " \"NAME-1\": \"RK Electrical Works\",\n",
      " \"NAME-2\": \"RK Electrical Works\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"PUSA ROAD,NEW DELHI-110005\",\n",
      " \"ADDRESS-3\": \"A-2040 Palam Vinar. Dehi\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\",\n",
      " \"NAME-1\": \"RK Electrical Works\",\n",
      " \"NAME-2\": \"RK Electrical Works\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"PUSA ROAD,NEW DELHI-110005\",\n",
      " \"ADDRESS-3\": \"A-2040 Palam Vinar. Dehi\",\n",
      " \"ADDRESS-4\": \"|A-2040 Palam Vinar. Dehi\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\",\n",
      " \"NAME-1\": \"RK Electrical Works\",\n",
      " \"NAME-2\": \"RK Electrical Works\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"PUSA ROAD,NEW DELHI-110005\",\n",
      " \"ADDRESS-3\": \"A-2040 Palam Vinar. Dehi\",\n",
      " \"ADDRESS-4\": \"|A-2040 Palam Vinar. Dehi\",\n",
      " \"DESCRIPTION\": \"Professional Services\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\",\n",
      " \"NAME-1\": \"RK Electrical Works\",\n",
      " \"NAME-2\": \"RK Electrical Works\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"PUSA ROAD,NEW DELHI-110005\",\n",
      " \"ADDRESS-3\": \"A-2040 Palam Vinar. Dehi\",\n",
      " \"ADDRESS-4\": \"|A-2040 Palam Vinar. Dehi\",\n",
      " \"AMOUNT\": 100000.0,\n",
      " \"DESCRIPTION\": \"Professional Services\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\",\n",
      " \"NAME-1\": \"RK Electrical Works\",\n",
      " \"NAME-2\": \"RK Electrical Works\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"PUSA ROAD,NEW DELHI-110005\",\n",
      " \"ADDRESS-3\": \"A-2040 Palam Vinar. Dehi\",\n",
      " \"ADDRESS-4\": \"|A-2040 Palam Vinar. Dehi\",\n",
      " \"AMOUNT\": 100000.0,\n",
      " \"DESCRIPTION\": \"Professional Services\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\",\n",
      " \"NAME-1\": \"RK Electrical Works\",\n",
      " \"NAME-2\": \"RK Electrical Works\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"PUSA ROAD,NEW DELHI-110005\",\n",
      " \"ADDRESS-3\": \"A-2040 Palam Vinar. Dehi\",\n",
      " \"ADDRESS-4\": \"|A-2040 Palam Vinar. Dehi\",\n",
      " \"AMOUNT\": 100000.0,\n",
      " \"DESCRIPTION\": \"Professional Services\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\",\n",
      " \"NAME-1\": \"RK Electrical Works\",\n",
      " \"NAME-2\": \"RK Electrical Works\",\n",
      " \"RATE\": \"9%\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"PUSA ROAD,NEW DELHI-110005\",\n",
      " \"ADDRESS-3\": \"A-2040 Palam Vinar. Dehi\",\n",
      " \"ADDRESS-4\": \"|A-2040 Palam Vinar. Dehi\",\n",
      " \"AMOUNT\": 100000.0,\n",
      " \"DESCRIPTION\": \"Professional Services\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\",\n",
      " \"NAME-1\": \"RK Electrical Works\",\n",
      " \"NAME-2\": \"RK Electrical Works\",\n",
      " \"RATE\": \"9%\",\n",
      " \"RATE-5\": \"9%\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"PUSA ROAD,NEW DELHI-110005\",\n",
      " \"ADDRESS-3\": \"A-2040 Palam Vinar. Dehi\",\n",
      " \"ADDRESS-4\": \"|A-2040 Palam Vinar. Dehi\",\n",
      " \"AMOUNT\": 100000.0,\n",
      " \"DESCRIPTION\": \"Professional Services\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\",\n",
      " \"NAME-1\": \"RK Electrical Works\",\n",
      " \"NAME-2\": \"RK Electrical Works\",\n",
      " \"RATE\": \"9%\",\n",
      " \"RATE-5\": \"9%\",\n",
      " \"RATE-6\": \"9%\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"PUSA ROAD,NEW DELHI-110005\",\n",
      " \"ADDRESS-3\": \"A-2040 Palam Vinar. Dehi\",\n",
      " \"ADDRESS-4\": \"|A-2040 Palam Vinar. Dehi\",\n",
      " \"AMOUNT\": 100000.0,\n",
      " \"DESCRIPTION\": \"Professional Services\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"CA SAKSHI AND ASSOCIATES\",\n",
      " \"NAME-1\": \"RK Electrical Works\",\n",
      " \"NAME-2\": \"RK Electrical Works\",\n",
      " \"RATE\": \"9%\",\n",
      " \"RATE-5\": \"9%\",\n",
      " \"RATE-6\": \"9%\",\n",
      " \"RATE-7\": \"9\"\n",
      "}\n",
      "<class 'dict'>\n",
      "Moved file to output.html\n",
      "#######preview-page0.jpg#######\n",
      "Processed text:\n",
      "{\n",
      " \"Items\": [],\n",
      " \"RATE\": \"0.00%\"\n",
      "}\n",
      "<class 'dict'>\n",
      "Moved file to output.html\n",
      "#######professional-invoice-template-1778abbd1062a26ae87383dc4333678e_og.png#######\n",
      "Processed text:\n",
      "{\n",
      " \"INVOICE/BILL_NO\": \"INV-0012\",\n",
      " \"Items\": []\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"DATE\": \"13/7/2020\",\n",
      " \"INVOICE/BILL_NO\": \"INV-0012\",\n",
      " \"Items\": []\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"DATE\": \"13/7/2020\",\n",
      " \"INVOICE/BILL_NO\": \"INV-0012\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Richard Glenn\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"DATE\": \"13/7/2020\",\n",
      " \"INVOICE/BILL_NO\": \"INV-0012\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Richard Glenn\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-1\": \"New York, NY, 281502\",\n",
      " \"DATE\": \"13/7/2020\",\n",
      " \"INVOICE/BILL_NO\": \"INV-0012\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Richard Glenn\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-1\": \"New York, NY, 281502\",\n",
      " \"DATE\": \"13/7/2020\",\n",
      " \"INVOICE/BILL_NO\": \"INV-0012\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Richard Glenn\",\n",
      " \"NAME-2\": \"Richard Glenn\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-1\": \"New York, NY, 281502\",\n",
      " \"ADDRESS-3\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"DATE\": \"13/7/2020\",\n",
      " \"INVOICE/BILL_NO\": \"INV-0012\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Richard Glenn\",\n",
      " \"NAME-2\": \"Richard Glenn\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-1\": \"New York, NY, 281502\",\n",
      " \"ADDRESS-3\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-4\": \"New York, NY, 281562\\n\\nQuantity\\n\\nCotton Male T-shirt\",\n",
      " \"DATE\": \"13/7/2020\",\n",
      " \"INVOICE/BILL_NO\": \"INV-0012\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Richard Glenn\",\n",
      " \"NAME-2\": \"Richard Glenn\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-1\": \"New York, NY, 281502\",\n",
      " \"ADDRESS-3\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-4\": \"New York, NY, 281562\\n\\nQuantity\\n\\nCotton Male T-shirt\",\n",
      " \"AMOUNT\": 84.0,\n",
      " \"DATE\": \"13/7/2020\",\n",
      " \"INVOICE/BILL_NO\": \"INV-0012\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Richard Glenn\",\n",
      " \"NAME-2\": \"Richard Glenn\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-1\": \"New York, NY, 281502\",\n",
      " \"ADDRESS-3\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-4\": \"New York, NY, 281562\\n\\nQuantity\\n\\nCotton Male T-shirt\",\n",
      " \"AMOUNT\": 42.0,\n",
      " \"DATE\": \"13/7/2020\",\n",
      " \"INVOICE/BILL_NO\": \"INV-0012\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Richard Glenn\",\n",
      " \"NAME-2\": \"Richard Glenn\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-1\": \"New York, NY, 281502\",\n",
      " \"ADDRESS-3\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-4\": \"New York, NY, 281562\\n\\nQuantity\\n\\nCotton Male T-shirt\",\n",
      " \"AMOUNT\": 84.0,\n",
      " \"DATE\": \"13/7/2020\",\n",
      " \"INVOICE/BILL_NO\": \"INV-0012\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Richard Glenn\",\n",
      " \"NAME-2\": \"Richard Glenn\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-1\": \"New York, NY, 281502\",\n",
      " \"ADDRESS-3\": \"4275 Crummit Lane, Happy Village\",\n",
      " \"ADDRESS-4\": \"New York, NY, 281562\\n\\nQuantity\\n\\nCotton Male T-shirt\",\n",
      " \"AMOUNT\": 5200.0,\n",
      " \"DATE\": \"13/7/2020\",\n",
      " \"INVOICE/BILL_NO\": \"INV-0012\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Richard Glenn\",\n",
      " \"NAME-2\": \"Richard Glenn\"\n",
      "}\n",
      "<class 'dict'>\n",
      "Moved file to output.html\n",
      "#######18102022_TAX_INVOICE_IN00012_ClearTax-11-1024x1012 (1).jpg#######\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text:\n",
      "{\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Shri Ganesh Catering Services\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"Kabir Jewellers_P\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Shri Ganesh Catering Services\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"Kabir Jewellers_P\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Shri Ganesh Catering Services\",\n",
      " \"NAME-1\": \"Kabir Jewellers_AP\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"Kabir Jewellers_P\",\n",
      " \"ADDRESS-2\": \"Road, Hyderabad\\n\\nTotal 34,970.00\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Shri Ganesh Catering Services\",\n",
      " \"NAME-1\": \"Kabir Jewellers_AP\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"Kabir Jewellers_P\",\n",
      " \"ADDRESS-2\": \"Road, Hyderabad\\n\\nTotal 34,970.00\",\n",
      " \"DATE\": \"13/07/2017\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Shri Ganesh Catering Services\",\n",
      " \"NAME-1\": \"Kabir Jewellers_AP\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"Kabir Jewellers_P\",\n",
      " \"ADDRESS-2\": \"Road, Hyderabad\\n\\nTotal 34,970.00\",\n",
      " \"DATE\": \"13/07/2017\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Shri Ganesh Catering Services\",\n",
      " \"NAME-1\": \"Kabir Jewellers_AP\",\n",
      " \"NAME-3\": \"Kabir Jewellers_AP\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"Kabir Jewellers_P\",\n",
      " \"ADDRESS-2\": \"Road, Hyderabad\\n\\nTotal 34,970.00\",\n",
      " \"ADDRESS-4\": \".45 Airport Road, Hyderabad\\n\\nAndhra Pradesh Andhra Pradesh\\nCountry of Supply\",\n",
      " \"DATE\": \"13/07/2017\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Shri Ganesh Catering Services\",\n",
      " \"NAME-1\": \"Kabir Jewellers_AP\",\n",
      " \"NAME-3\": \"Kabir Jewellers_AP\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"Kabir Jewellers_P\",\n",
      " \"ADDRESS-2\": \"Road, Hyderabad\\n\\nTotal 34,970.00\",\n",
      " \"ADDRESS-4\": \".45 Airport Road, Hyderabad\\n\\nAndhra Pradesh Andhra Pradesh\\nCountry of Supply\",\n",
      " \"DATE\": \"13/07/2017\",\n",
      " \"DATE-5\": \"12/07/2017\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Shri Ganesh Catering Services\",\n",
      " \"NAME-1\": \"Kabir Jewellers_AP\",\n",
      " \"NAME-3\": \"Kabir Jewellers_AP\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{\n",
      " \"ADDRESS\": \"Kabir Jewellers_P\",\n",
      " \"ADDRESS-2\": \"Road, Hyderabad\\n\\nTotal 34,970.00\",\n",
      " \"ADDRESS-4\": \".45 Airport Road, Hyderabad\\n\\nAndhra Pradesh Andhra Pradesh\\nCountry of Supply\",\n",
      " \"DATE\": \"13/07/2017\",\n",
      " \"DATE-5\": \"12/07/2017\",\n",
      " \"DESCRIPTION\": \"saffron\",\n",
      " \"Items\": [],\n",
      " \"NAME\": \"Shri Ganesh Catering Services\",\n",
      " \"NAME-1\": \"Kabir Jewellers_AP\",\n",
      " \"NAME-3\": \"Kabir Jewellers_AP\"\n",
      "}\n",
      "<class 'dict'>\n",
      "Moved file to output.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\")  # load the spacy model\n",
    "\n",
    "def process_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    return pytesseract.image_to_string(thresh, lang='eng')\n",
    "\n",
    "def process_file(file_path):\n",
    "    text = \"\"\n",
    "    if file_path.endswith(\".jpg\") or file_path.endswith(\".jpeg\") or file_path.endswith(\".png\"):\n",
    "        im = cv2.imread(file_path)\n",
    "        text = process_image(im)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        images = convert_from_path(file_path)\n",
    "        for i, image in enumerate(images):\n",
    "            image.save(f'special/page_{i+1}.jpg', 'JPEG')\n",
    "            text += process_image(image)\n",
    "    else:\n",
    "        print(\"Input the correct file\")\n",
    "    return text\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    rows = []\n",
    "    for root, dirs, filenames in os.walk(input_dir):\n",
    "        for filename in filenames:\n",
    "            print('#######' + filename + '#######')\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            text = process_file(input_path)\n",
    "            print(\"Processed text:\")\n",
    "#             print(text)\n",
    "#             doc = nlp(text)\n",
    "#             mylist = []\n",
    "#             for ent in doc.ents:\n",
    "#                 print(ent.text, ent.label_)\n",
    "#                 mylist.append([ent.text,ent.label_])\n",
    "#             print(mylist)\n",
    "            doc = nlp(text)\n",
    "            max_amt = 0\n",
    "            i = 1\n",
    "            data = {}\n",
    "            items_list = []\n",
    "              # Iterating over every entitiy to create a dictionary\n",
    "            for ent in doc.ents:\n",
    "                # Saving only one instance of Total Bill Amount\n",
    "                if (ent.label_ == \"AMOUNT\"):\n",
    "                    try:\n",
    "                        amt = float(ent.text)\n",
    "                        if amt > max_amt:\n",
    "                            data[\"AMOUNT\"] = amt\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                # Creating a list of Items\n",
    "                elif (ent.label_ == \"Items\"):\n",
    "                    try:\n",
    "                        items_list.append(ent.text)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                # Checking if the detected key is already present in the key,\n",
    "                # If yes then we create a new key to store that value instead of overwriting the previous one\n",
    "                else:\n",
    "                    if ent.label_ in data.keys():\n",
    "                        data[ent.label_+\"-\"+str(i)] = ent.text\n",
    "                        i +=1\n",
    "                    else:\n",
    "                        data[ent.label_] = ent.text\n",
    "                      # Staring the list of items using the Items key in the dictionary\n",
    "                data[\"Items\"]=items_list\n",
    "                      # Sorting all the elements of the dictionary\n",
    "                data = dict(sorted(data.items()))\n",
    "              # Printing final result\n",
    "                print(json.dumps(data, indent=1))\n",
    "                print(type(data))\n",
    "#             type_bill = ','.join(i[0] for i in mylist if i[1] == 'TYPE_OF_BILL')\n",
    "#             name = ','.join(i[0] for i in mylist if i[1] == 'NAME')\n",
    "#             invoice = ','.join(i[0] for i in mylist if i[1] == 'INVOICE_NO./BILL_NO.')\n",
    "#             email = ','.join(i[0] for i in mylist if i[1] == 'EMAIL')\n",
    "#             date = ','.join(i[0] for i in mylist if i[1] == 'ORDER_DATE')\n",
    "#             description = ','.join(i[0] for i in mylist if i[1] == 'DESCRIPTION')\n",
    "#             amount = ','.join(i[0] for i in mylist if i[1] == 'TOTAL_AMOUNT')\n",
    "#             tax = ','.join(i[0] for i in mylist if i[1] == 'TAX_RATE')\n",
    "#             quantity = ','.join(i[0] for i in mylist if i[1] == 'QUANTITY')\n",
    "#             mobile = ','.join(i[0] for i in mylist if i[1] == 'MOBILE_No.')\n",
    "#             df = pd.DataFrame(columns=['TYPE_OF_BILL', 'NAME', 'INVOICE_NO./BILL_NO.', 'EMAIL', 'ORDER_DATE', 'DESCRIPTION', 'TOTAL_AMOUNT', 'TAX_RATE', 'QUANTITY', 'MOBILE_NO.'])\n",
    "#             df = df.append({'TYPE_OF_BILL':type_bill,'NAME':name,'INVOICE_NO./BILL_NO.':invoice,'EMAIL':email,'ORDER_DATE':date,'DESCRIPTION':description,'TOTAL_AMOUNT':amount,'TAX_RATE':tax,'QUANTITY':quantity,'MOBILE_NO.':mobile}, ignore_index=True)\n",
    "#             print(df)\n",
    "#             html_table = df.to_html()\n",
    "\n",
    "            # write the HTML table to a file\n",
    "#             with open('output.html', 'w') as f:\n",
    "#                 f.write(html_table)\n",
    "            output_path = \"output.html\"\n",
    "            shutil.move(input_path, output_path)\n",
    "            print(f\"Moved file to {output_path}\")\n",
    "            \n",
    "            \n",
    "process_directory(r\"/home/sunil/Desktop/np/uploads/\", r\"/home/sunil/Desktop/np/special/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cf928db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Receipt TYPE_OF_BILL\n",
      "04/13/2020 DATE\n",
      "AUTOTECH SERVICE CENTER NAME\n",
      "8980 West Lake Pond Dr. Long Beach, CA 90813 ADDRESS\n",
      "Michael Carlsmilth NAME\n",
      "(958) 2035829 MOBILE_NO\n",
      "michael@example.com EMAIL\n",
      "123 Rodeo Drive, 213 Mark Street ADDRESS\n",
      "Great City, Some State, 1111 ADDRESS\n",
      "United States COUNTRY\n",
      "Apr 13,2020 DATE\n",
      "04/13/2021 DATE\n",
      "Tie Rod Ends DESCRIPTION\n",
      "Idler Arm Belts DESCRIPTION\n",
      "Spark plugs DESCRIPTION\n",
      "Oil DESCRIPTION\n",
      "Brakes Bearing and Shocks DESCRIPTION\n",
      "Wheel Cylinder DESCRIPTION\n",
      "Materials Labor DESCRIPTION\n",
      "Oil Filter DESCRIPTION\n",
      "Synthetic Ol DESCRIPTION\n",
      "200 AMOUNT\n",
      "belt replacement DESCRIPTION\n",
      "100 AMOUNT\n",
      "Spark plugs DESCRIPTION\n",
      "470 AMOUNT\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m quantity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m mylist \u001b[38;5;28;01mif\u001b[39;00m i[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQUANTITY\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     51\u001b[0m mobile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m mylist \u001b[38;5;28;01mif\u001b[39;00m i[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMOBILE_No.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 53\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTYPE_OF_BILL\u001b[39m\u001b[38;5;124m'\u001b[39m:type_bill,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNAME\u001b[39m\u001b[38;5;124m'\u001b[39m:name,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mINVOICE_NO./BILL_NO.\u001b[39m\u001b[38;5;124m'\u001b[39m:invoice,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEMAIL\u001b[39m\u001b[38;5;124m'\u001b[39m:email,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mORDER_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m:date,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDESCRIPTION\u001b[39m\u001b[38;5;124m'\u001b[39m:description,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTOTAL_AMOUNT\u001b[39m\u001b[38;5;124m'\u001b[39m:amount,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTAX_RATE\u001b[39m\u001b[38;5;124m'\u001b[39m:tax,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQUANTITY\u001b[39m\u001b[38;5;124m'\u001b[39m:quantity,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMOBILE_NO.\u001b[39m\u001b[38;5;124m'\u001b[39m:mobile}, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('model-best') \n",
    "sentence = \"\"\"g Service Receipt\n",
    "\n",
    "04/13/2020\n",
    "AUTOTECH SERVICE CENTER - 8980 West Lake Pond Dr. Long Beach, CA 90813\n",
    "Bill To\n",
    "Michael Carlsmilth\n",
    "(958) 2035829\n",
    "michael@example.com\n",
    "123 Rodeo Drive, 213 Mark Street\n",
    "Great City, Some State, 1111\n",
    "United States.\n",
    "Car Make Car Model YearModel  Color Transmission Engine Type\n",
    "BMW 3161 1999 Red  Automatic Gas\n",
    "Last Change Oil Mileage Reading Next Change Ol\n",
    "Apr 13,2020 99000 04/13/2021\n",
    "Under Chassis Hood\n",
    "Tie Rod Ends, Idler Arm Belts, Spark plugs, Oil, ATF\n",
    "Brakes Bearing and Shocks\n",
    "Wheel Cylinder\n",
    "Materials Labor\n",
    "Particulars Amount Particulars Amount\n",
    "Oil Filter 20 Change Oil 50\n",
    "Synthetic Ol 200 belt replacement 100\n",
    "Spark plugs 100\n",
    "\n",
    "Total Labor Cost $150\n",
    "\n",
    "Total Material Cost  $320\n",
    "\n",
    "Total Cost $470\"\"\"\n",
    "doc = nlp(sentence) \n",
    "\n",
    "mylist= []\n",
    "for ent in doc.ents: \n",
    "    print(ent.text, ent.label_)\n",
    "    mylist.append([ent.text,ent.label_])\n",
    "\n",
    "# print(mylist)\n",
    "\n",
    "type_bill = ','.join(i[0] for i in mylist if i[1] == 'TYPE_OF_BILL')\n",
    "name = ','.join(i[0] for i in mylist if i[1] == 'NAME')\n",
    "invoice = ','.join(i[0] for i in mylist if i[1] == 'INVOICE_NO./BILL_NO.')\n",
    "email = ','.join(i[0] for i in mylist if i[1] == 'EMAIL')\n",
    "date = ','.join(i[0] for i in mylist if i[1] == 'ORDER_DATE')\n",
    "description = ','.join(i[0] for i in mylist if i[1] == 'DESCRIPTION')\n",
    "amount = ','.join(i[0] for i in mylist if i[1] == 'TOTAL_AMOUNT')\n",
    "tax = ','.join(i[0] for i in mylist if i[1] == 'TAX_RATE')\n",
    "quantity = ','.join(i[0] for i in mylist if i[1] == 'QUANTITY')\n",
    "mobile = ','.join(i[0] for i in mylist if i[1] == 'MOBILE_No.')\n",
    "\n",
    "df = df.append({'TYPE_OF_BILL':type_bill,'NAME':name,'INVOICE_NO./BILL_NO.':invoice,'EMAIL':email,'ORDER_DATE':date,'DESCRIPTION':description,'TOTAL_AMOUNT':amount,'TAX_RATE':tax,'QUANTITY':quantity,'MOBILE_NO.':mobile}, ignore_index=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f675de",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = df.to_html()\n",
    "\n",
    "#write html to file\n",
    "text_file = open(\"output.html\", \"w\")\n",
    "text_file.write(html)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3698ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\")  # load the spacy model\n",
    "\n",
    "def process_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    return pytesseract.image_to_string(thresh, lang='eng')\n",
    "\n",
    "def process_file(file_path):\n",
    "    text = \"\"\n",
    "    if file_path.endswith(\".jpg\") or file_path.endswith(\".jpeg\") or file_path.endswith(\".png\"):\n",
    "        im = cv2.imread(file_path)\n",
    "        text = process_image(im)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        images = convert_from_path(file_path)\n",
    "        for i, image in enumerate(images):\n",
    "            image.save(f'special/page_{i+1}.jpg', 'JPEG')\n",
    "            text += process_image(image)\n",
    "    else:\n",
    "        print(\"Input the correct file\")\n",
    "    return text\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    rows = []\n",
    "    df = pd.DataFrame(columns=['TYPE_OF_BILL', 'NAME', 'INVOICE_NO./BILL_NO.', 'EMAIL', 'ORDER_DATE', 'DESCRIPTION', 'TOTAL_AMOUNT', 'TAX_RATE', 'QUANTITY', 'MOBILE_NO.'])\n",
    "    for root, dirs, filenames in os.walk(input_dir):\n",
    "        for filename in filenames:\n",
    "            print('#######' + filename + '#######')\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            text = process_file(input_path)\n",
    "            print(\"Processed text:\")\n",
    "#             print(text)\n",
    "            doc = nlp(text)\n",
    "            mylist = []\n",
    "            for ent in doc.ents:\n",
    "                print(ent.text, ent.label_)\n",
    "                mylist.append([ent.text,ent.label_])\n",
    "            print(type(mylist))\n",
    "            type_bill = ','.join(i[0] for i in mylist if i[1] == 'TYPE_OF_BILL')\n",
    "            name = ','.join(i[0] for i in mylist if i[1] == 'NAME')\n",
    "            invoice = ','.join(i[0] for i in mylist if i[1] == 'INVOICE_NO./BILL_NO.')\n",
    "            email = ','.join(i[0] for i in mylist if i[1] == 'EMAIL')\n",
    "            date = ','.join(i[0] for i in mylist if i[1] == 'ORDER_DATE')\n",
    "            description = ','.join(i[0] for i in mylist if i[1] == 'DESCRIPTION')\n",
    "            amount = ','.join(i[0] for i in mylist if i[1] == 'TOTAL_AMOUNT')\n",
    "            tax = ','.join(i[0] for i in mylist if i[1] == 'TAX_RATE')\n",
    "            quantity = ','.join(i[0] for i in mylist if i[1] == 'QUANTITY')\n",
    "            mobile = ','.join(i[0] for i in mylist if i[1] == 'MOBILE_No.')\n",
    "            df = df.append({'TYPE_OF_BILL':type_bill,'NAME':name,'INVOICE_NO./BILL_NO.':invoice,'EMAIL':email,'ORDER_DATE':date,'DESCRIPTION':description,'TOTAL_AMOUNT':amount,'TAX_RATE':tax,'QUANTITY':quantity,'MOBILE_NO.':mobile}, ignore_index=True)\n",
    "            print(type(df))\n",
    "            output_path = \"output.html\"\n",
    "            shutil.move(input_path, output_path)\n",
    "            print(f\"Moved file to {output_path}\")\n",
    "    return df\n",
    "process_directory(r\"/home/sunil/Desktop/np/uploads/\", r\"/home/sunil/Desktop/np/special/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4128b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# Define output folder to save new model\n",
    "# model_dir = 'D:/Anindya/E/model'\n",
    "model_dir = '/home/sunil/Desktop/np/annotations.json'\n",
    "# Train new NER model\n",
    "def train_new_NER(model=None, output_dir=model_dir, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly  but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "            \n",
    "# Finally train the model by calling above function\n",
    "train_new_NER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3004dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model_dir = 'D:/Anindya/E/updated_model'\n",
    "\n",
    "## Update existing spacy model and store into a folder\n",
    "def update_model(model='en_core_web_sm', output_dir=updated_model_dir, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly  but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Finally train the model by calling above function        \n",
    "update_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c967b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    # return pytesseract.image_to_string(thresh, lang='eng')\n",
    "    image_text=pytesseract.image_to_string(thresh, lang='eng')\n",
    "    return image_text \n",
    "def get_data():\n",
    "    alpha = process_image()\n",
    "    print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "617f5add",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_data() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_data() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "get_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27459143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89338787",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alpha' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 46\u001b[0m\n\u001b[1;32m      3\u001b[0m alha \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mRestaurant Receipt\u001b[39m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mName Jane Smith 002\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# filtered_entities = {}\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# for ent in doc.ents:\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#     if ent.label_ in [\"TYPE_OF_BILL\",\"DATE\",\"MOBILE_NO\",\"ADDRESS\",\"DESCRIPTION\",\"STATE\",\"INVOICE/BILL_NO\",\"EMAIL\",\"TIME\",\"AMOUNT\",\"RATE\",\"QUANTITY\",\"NAME\",\"COUNTRY\"]:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# print(filtered_entities)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# entities= []\u001b[39;00m\n\u001b[1;32m     45\u001b[0m output \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 46\u001b[0m     \u001b[43malpha\u001b[49m,\n\u001b[1;32m     47\u001b[0m     {\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m:[]\n\u001b[1;32m     49\u001b[0m     }\n\u001b[1;32m     50\u001b[0m     \n\u001b[1;32m     51\u001b[0m ]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ent\u001b[38;5;241m.\u001b[39mlabel_ \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTYPE_OF_BILL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMOBILE_NO\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mADDRESS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDESCRIPTION\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTATE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINVOICE/BILL_NO\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEMAIL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTIME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMOUNT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRATE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQUANTITY\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOUNTRY\u001b[39m\u001b[38;5;124m\"\u001b[39m] :\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alpha' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\") \n",
    "alha = \"\"\"Restaurant Receipt\n",
    "\n",
    "Name Jane Smith 002\n",
    "\n",
    "Receipt #\n",
    "\n",
    "Email jasmith@noemail.com  Date 4/8/2019\n",
    "\n",
    "Phone Number (123) 123-4567 Time 5:00 PM\n",
    "\n",
    "Ordered Items\n",
    "\n",
    "Food Name Quantity Price Total\n",
    "\n",
    "1 Food A 5 5 25\n",
    "\n",
    "2 Food B 5 5 25\n",
    "\n",
    "3 Food C 5 5 25\n",
    "\n",
    "4 Drink D 5 2 10\n",
    "\n",
    "5 Drink E 5 2 10\n",
    "Subtotal $95\n",
    "Tax % 5\n",
    "\n",
    "I\n",
    "\n",
    "Total $99.75\n",
    "\n",
    "Hope you enjoy your dining experience!\n",
    "\n",
    "\"\"\"\n",
    "# filtered_entities = {}\n",
    "# for ent in doc.ents:\n",
    "#     if ent.label_ in [\"TYPE_OF_BILL\",\"DATE\",\"MOBILE_NO\",\"ADDRESS\",\"DESCRIPTION\",\"STATE\",\"INVOICE/BILL_NO\",\"EMAIL\",\"TIME\",\"AMOUNT\",\"RATE\",\"QUANTITY\",\"NAME\",\"COUNTRY\"]:\n",
    "#         if ent.label_ not in filtered_entities:\n",
    "#             filtered_entities[ent.label_] = []\n",
    "#         filtered_entities[ent.label_].append(ent.text)\n",
    "\n",
    "# print(filtered_entities)\n",
    "# entities= []\n",
    "output = [\n",
    "    alpha,\n",
    "    {\n",
    "        \"entities\":[]\n",
    "    }\n",
    "    \n",
    "]\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ in [\"TYPE_OF_BILL\",\"DATE\",\"MOBILE_NO\",\"ADDRESS\",\"DESCRIPTION\",\"STATE\",\"INVOICE/BILL_NO\",\"EMAIL\",\"TIME\",\"AMOUNT\",\"RATE\",\"QUANTITY\",\"NAME\",\"COUNTRY\"] :\n",
    "        output[1]['entities'].append([ent.start_char, ent.end_char, ent.label_])\n",
    "    \n",
    "print(output)\n",
    "# to get the data in required json format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac72f627",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m         file\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(file_data,file,indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\n\u001b[1;32m     14\u001b[0m write_json(y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# with open('/home/sunil/Desktop/np/annotations.json', 'r') as f:\n",
    "# #     TEST_DATA = json.load(f)\n",
    "#     TRAIN_DATA = json.load(f)#         file_data[\"annotations\"][0].append(new_data)\n",
    "def write_json(new_data,filename='annotations.json'):\n",
    "    with open(filename, 'r+') as file:\n",
    "        file_data = json.load(file)\n",
    "        file_data.update({\"annotations\":[new_data]})\n",
    "        file.seek(1)\n",
    "        json.dump(file_data,file,indent=4)\n",
    "    \n",
    "        \n",
    "y = output\n",
    "write_json(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db338c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_json(new_data, filename='data.json'):\n",
    "#     with open(filename,'r+') as file:\n",
    "#           # First we load existing data into a dict.\n",
    "#         file_data = json.load(file)\n",
    "#         # Join new_data with file_data inside emp_details\n",
    "#         file_data[\"emp_details\"].append(new_data)\n",
    "#         # Sets file's current position at offset.\n",
    "#         file.seek(0)\n",
    "#         # convert back to json.\n",
    "#         json.dump(file_data, file, indent = 4)\n",
    " \n",
    "#     # python object to be appended\n",
    "# y = {\"emp_name\":\"Nikhil\",\n",
    "#      \"email\": \"nikhil@geeksforgeeks.org\",\n",
    "#      \"job_profile\": \"Full Time\"\n",
    "#     }\n",
    "     \n",
    "# write_json(y)\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e08446aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunil/.local/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# load the training data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_train_data\u001b[49m()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# create new Examples from the training data\u001b[39;00m\n\u001b[1;32m     12\u001b[0m examples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# load the existing NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# load the training data\n",
    "train_data = load_train_data()\n",
    "\n",
    "# create new Examples from the training data\n",
    "examples = []\n",
    "for text, annotations in train_data:\n",
    "    example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "    examples.append(example)\n",
    "\n",
    "# train the NER model with the new data\n",
    "nlp.disable_pipes(\"tagger\", \"parser\")\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(10):\n",
    "    losses = {}\n",
    "    batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        nlp.update(batch, sgd=optimizer, losses=losses)\n",
    "    print(\"Losses\", losses)\n",
    "\n",
    "# save the updated model\n",
    "nlp.to_disk(\"updated_ner_model\")                                                                                                        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc55f7",
   "metadata": {},
   "source": [
    "\n",
    "# REINFORCEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained spaCy NER model\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\")\n",
    "\n",
    "# Define the labels for named entities\n",
    "LABELS = [\"TYPE_OF_BILL\",\"DATE\",\"MOBILE_NO\",\"ADDRESS\",\"DESCRIPTION\",\"STATE\",\"INVOICE/BILL_NO\",\"EMAIL\",\"TIME\",\"AMOUNT\",\"RATE\",\"QUANTITY\",\"NAME\",\"COUNTRY\"]\n",
    "\n",
    "# Define the Q-table\n",
    "q_table = np.zeros((len(LABELS), len(LABELS)))\n",
    "\n",
    "# Define the learning rate and discount factor\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "# Define a function to extract features from a sentence\n",
    "def extract_features(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    features = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in LABELS:\n",
    "            features.append(token.ent_type_)\n",
    "    return features\n",
    "\n",
    "# Define a function to choose an action based on the Q-table and exploration rate\n",
    "def choose_action(state, exploration_rate):\n",
    "    if random.uniform(0, 1) < exploration_rate:\n",
    "        return random.randint(0, len(LABELS) - 1)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# Define a function to update the Q-table based on a state, action, reward, and next state\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    old_value = q_table[state][action]\n",
    "    next_max = np.max(q_table[next_state])\n",
    "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "    q_table[state][action] = new_value\n",
    "\n",
    "# Train the model using reinforcement learning\n",
    "exploration_rate = 1.0\n",
    "for i in range(1000):\n",
    "    # Choose a sentence to train on\n",
    "    sentence = \"John works at Apple in California\"\n",
    "    features = extract_features(sentence)\n",
    "\n",
    "    # Choose an action based on the current state and exploration rate\n",
    "    state = features[0]\n",
    "    action = choose_action(state, exploration_rate)\n",
    "\n",
    "    # Apply the action and observe the next state and reward\n",
    "    next_state = LABELS[action]\n",
    "        if next_state in features:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = -1\n",
    "\n",
    "    # Update the Q-table based on the current state, action, reward, and next state\n",
    "    update_q_table(state, action, reward, next_state)\n",
    "\n",
    "    # Decay the exploration rate\n",
    "    exploration_rate *= 0.99\n",
    "\n",
    "# Use the Q-table to predict named entities in a new sentence\n",
    "sentence = \"\"\"Restaurant Receipt\n",
    "\n",
    "Name Jane Smith 002\n",
    "\n",
    "Receipt #\n",
    "\n",
    "Email jasmith@noemail.com  Date 4/8/2019\n",
    "\n",
    "Phone Number (123) 123-4567 Time 5:00 PM\n",
    "\n",
    "Ordered Items\n",
    "\n",
    "Food Name Quantity Price Total\n",
    "\n",
    "1 Food A 5 5 25\n",
    "\n",
    "2 Food B 5 5 25\n",
    "\n",
    "3 Food C 5 5 25\n",
    "\n",
    "4 Drink D 5 2 10\n",
    "\n",
    "5 Drink E 5 2 10\n",
    "Subtotal $95\n",
    "Tax % 5\n",
    "\n",
    "Total $99.75\n",
    "\"\"\"\n",
    "features = extract_features(sentence)\n",
    "state = features[0]\n",
    "action = np.argmax(q_table[state])\n",
    "prediction = LABELS[action]\n",
    "print(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c22515",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_index = LABELS.index(state)\n",
    "action = np.argmax(q_table[state_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f3834",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained spaCy NER model\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\")\n",
    "\n",
    "# Define the labels for named entities\n",
    "LABELS = [\"TYPE_OF_BILL\",\"DATE\",\"MOBILE_NO\",\"ADDRESS\",\"DESCRIPTION\",\"STATE\",\"INVOICE/BILL_NO\",\"EMAIL\",\"TIME\",\"AMOUNT\",\"RATE\",\"QUANTITY\",\"NAME\",\"COUNTRY\"]\n",
    "\n",
    "# Define the Q-table\n",
    "q_table = np.zeros((len(LABELS), len(LABELS)))\n",
    "\n",
    "# Define the learning rate and discount factor\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "# Define a function to extract features from a sentence\n",
    "def extract_features(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    features = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in LABELS:\n",
    "            features.append(token.ent_type_)\n",
    "    return features\n",
    "\n",
    "# Define a function to choose an action based on the Q-table and exploration rate\n",
    "def choose_action(state, exploration_rate):\n",
    "    if random.uniform(0, 1) < exploration_rate:\n",
    "        return random.randint(0, len(LABELS) - 1)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# Define a function to update the Q-table based on a state, action, reward, and next state\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    old_value = q_table[state][action]\n",
    "    next_max = np.max(q_table[next_state])\n",
    "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "    q_table[state][action] = new_value\n",
    "\n",
    "# Train the model using reinforcement learning\n",
    "exploration_rate = 1.0\n",
    "for i in range(1000):\n",
    "    # Choose a sentence to train on\n",
    "    sentence = \"John works at Apple in California\"\n",
    "    features = extract_features(sentence)\n",
    "\n",
    "    # Choose an action based on the current state and exploration rate\n",
    "    state = features[0]\n",
    "    action = choose_action(state, exploration_rate)\n",
    "\n",
    "    # Apply the action and observe the next state and reward\n",
    "    next_state = LABELS[action]\n",
    "    if next_state in features:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = -1\n",
    "\n",
    "    # Update the Q-table based on the current state, action, reward, and next state\n",
    "    if len(features) > 0:\n",
    "        state_index = LABELS.index(state)\n",
    "        next_state_index = LABELS.index(next_state)\n",
    "        update_q_table(state_index, action, reward, next_state_index)\n",
    "\n",
    "    # Decay the exploration rate\n",
    "    exploration_rate *= 0.99\n",
    "\n",
    "# Use the Q-table to predict named entities in a new sentence\n",
    "sentence = \"\"\"Restaurant Receipt\n",
    "\n",
    "Name Jane Smith 002\n",
    "\n",
    "Receipt #\n",
    "\n",
    "Email jasmith@noemail.com  Date 4/8/2019\n",
    "\n",
    "Phone Number (123) 123-4567 Time 5:00 PM\n",
    "\n",
    "\n",
    "Ordered Items\n",
    "\n",
    "Food Name Quantity Price Total\n",
    "\n",
    "1 Food A 5 5 25\n",
    "\n",
    "2 Food B\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON data\n",
    "data = json.loads('{\"_id\": {\"$oid\": \"644a0f6cb75860c78f61e664\"},\"TYPE_OF_BILL\": \"\",\"DATE\": \"21-Dec-20,20-Dec-20\",\"MOBILE_NO\": \"\",\"ADDRESS\": \"Surabhi Hardwares, Bangalore,Surabhi Hardwares, Bangalore\",\"DESCRIPTION\": \"\",\"STATE\": \"Karnataka,Karnataka,Karnataka\",\"INVOICE/BILL_NO\": \"SHB/456/20\",\"EMAIL\": \"\",\"TIME\": \"\",\"AMOUNT\": \"3,500.00,4,130.00,3,500.00,315.00,315.00\",\"RATE\": \"315.00,315.00,9%,9%\",\"QUANTITY\": \"\",\"NAME\": \"Kiran Enterprises,Kiran Enterprises\",\"COUNTRY\": \"\"}')\n",
    "\n",
    "# Define the environment as the set of states\n",
    "states = [data]\n",
    "\n",
    "# Define the actions as buy or not buy\n",
    "actions = ['buy', 'not_buy']\n",
    "\n",
    "# Define the Q-table as a dictionary\n",
    "Q = {}\n",
    "for state in states:\n",
    "    for action in actions:\n",
    "        Q[str(state), action] = 0\n",
    "\n",
    "# Define the reward function as profit earned from buying the product\n",
    "def reward(state, action):\n",
    "    if action == 'buy':\n",
    "        amount = state['AMOUNT']\n",
    "        rate = state['RATE']\n",
    "        amount_list = [float(x.replace(',', '')) for x in amount.split(',')]\n",
    "        rate_list = [float(x.replace('%', ''))/100 if x.endswith('%') else float(x) for x in rate.split(',')]\n",
    "        total_cost = sum([a*r for a, r in zip(amount_list, rate_list)])\n",
    "        return total_cost\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Define the learning rate and discount factor\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "# Run the Q-learning algorithm for a fixed number of episodes\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    # Choose a random initial state\n",
    "    state = np.random.choice(states)\n",
    "    while True:\n",
    "        # Choose an action using an epsilon-greedy policy\n",
    "        epsilon = 0.1\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = np.random.choice(actions)\n",
    "        else:\n",
    "            q_values = [Q[str(state), a] for a in actions]\n",
    "            action = actions[np.argmax(q_values)]\n",
    "        \n",
    "        # Calculate the reward for the chosen action\n",
    "        r = reward(state, action)\n",
    "        \n",
    "        # Transition to the next state based on the chosen action\n",
    "        next_state = np.random.choice(states)\n",
    "        \n",
    "        # Update the Q-table using the Q-learning update rule\n",
    "        # Calculate the maximum Q-value for the next state and all possible actions\n",
    "        q_values_next = [Q[str(next_state), a] for a in actions]\n",
    "        max_q_next = np.max(q_values_next)\n",
    "        \n",
    "        # Update the Q-value for the current state-action pair\n",
    "        Q[str(state), action] += alpha * (r + gamma * max_q_next - Q[str(state), action])\n",
    "        \n",
    "        # Transition to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if the episode has ended\n",
    "        if episode == num_episodes - 1:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5cacf0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "def process_image(image):\n",
    "    # Convert the image to grayscale\n",
    "    image = r'/home/sunil/Desktop/np/ocr_data/images.jpeg'\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply a bilateral filter to reduce noise and preserve edges\n",
    "    blur = cv2.bilateralFilter(gray, 9, 75, 75)\n",
    "    \n",
    "    # Apply adaptive thresholding to segment the foreground text from the background\n",
    "    thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "    \n",
    "    # Perform OCR using pytesseract\n",
    "    text = pytesseract.image_to_string(thresh, lang='eng')\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc0946f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 7\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_image\u001b[39m(image):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Convert the image to grayscale\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/sunil/Desktop/np/ocr_data/images.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGR2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Apply a bilateral filter to reduce noise and preserve edges\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     blur \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mbilateralFilter(gray, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m75\u001b[39m, \u001b[38;5;241m75\u001b[39m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.7.0) :-1: error: (-5:Bad argument) in function 'cvtColor'\n> Overload resolution failed:\n>  - src is not a numpy array, neither a scalar\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "process_image(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03d54857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "imagePath =  r'/home/sunil/Desktop/np/ocr_data/images.jpeg'\n",
    "img = cv2.imread(imagePath)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "text = pytesseract.image_to_string(gray, lang = 'eng')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb9d56fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tkinter (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tkinter\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0b4c81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read image\n",
    "image = cv2.imread('/home/sunil/Desktop/np/ocr_data/images.jpeg')\n",
    "\n",
    "# Create a kernel for sharpening\n",
    "kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "\n",
    "# Apply the kernel to the image using the cv2.filter2D function\n",
    "sharpened_image = cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "# Display the original and sharpened images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Sharpened Image', sharpened_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d662ee24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9823917c",
   "metadata": {},
   "source": [
    "# JUST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91b42230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('annotations.json', 'r') as f:\n",
    "  DATA = json.load(f)[\"annotations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be5bb781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N = len(DATA)\n",
    "# Randomly select 20% of the data for testing\n",
    "test_idx = np.random.randint(N, size=N//5)\n",
    "TEST_DATA = np.array(DATA)[test_idx].tolist()\n",
    "# Leave the remaining 80% as training data\n",
    "train_idx = list(set(np.arange(N))-set(test_idx))\n",
    "TRAIN_DATA = np.array(DATA)[train_idx].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594ac1a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E203] If the tok2vec embedding layer is not updated during training, make sure to include it in 'annotating components'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m         doc \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mmake_doc(text)\n\u001b[1;32m     27\u001b[0m         example \u001b[38;5;241m=\u001b[39m Example\u001b[38;5;241m.\u001b[39mfrom_dict(doc, annotations)\n\u001b[0;32m---> 28\u001b[0m         \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m[\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dropout - make it harder to memorise data\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLosses\u001b[39m\u001b[38;5;124m\"\u001b[39m, losses)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/spacy/language.py:1155\u001b[0m, in \u001b[0;36mLanguage.update\u001b[0;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, proc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline:\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;66;03m# ignore statements are used here because mypy ignores hasattr\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1155\u001b[0m         \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msgd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sgd \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1158\u001b[0m             name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude\n\u001b[1;32m   1159\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proc, ty\u001b[38;5;241m.\u001b[39mTrainableComponent)\n\u001b[1;32m   1160\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mis_trainable\n\u001b[1;32m   1161\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1162\u001b[0m         ):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/spacy/pipeline/transition_parser.pyx:405\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.update\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/thinc/model.py:309\u001b[0m, in \u001b[0;36mModel.begin_update\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbegin_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable[[OutT], InT]]:\n\u001b[1;32m    303\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run the model over a batch of data, returning the output and a\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m    callback to complete the backward pass. A tuple (Y, finish_update),\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m    where Y is a batch of output data, and finish_update is a callback that\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;03m    takes the gradient with respect to the output and an optimizer function,\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m    and returns the gradient with respect to the input.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/spacy/ml/tb_framework.py:33\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(model, X, is_train):\n\u001b[0;32m---> 33\u001b[0m     step_model \u001b[38;5;241m=\u001b[39m \u001b[43mParserStepModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43munseen_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munseen_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_upper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhas_upper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_model, step_model\u001b[38;5;241m.\u001b[39mfinish_steps\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/spacy/ml/parser_model.pyx:213\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/spacy/pipeline/tok2vec.py:292\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, inputs, is_train)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE203\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtok2vec\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    294\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mtensor)\n",
      "\u001b[0;31mValueError\u001b[0m: [E203] If the tok2vec embedding layer is not updated during training, make sure to include it in 'annotating components'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('/home/sunil/Desktop/np/model-best') # Spanish\n",
    "\n",
    "# Getting the pipeline component\n",
    "ner=nlp.get_pipe(\"ner\")\n",
    "# Disable pipeline components you dont need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy.training.example import Example\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "  # Training for 30 iterations\n",
    "  for iteration in range(30):\n",
    "    # shuffling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "      for text, annotations in batch:\n",
    "        # create Example\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        nlp.update(\n",
    "                 [example],\n",
    "                 drop=0.5, # dropout - make it harder to memorise data\n",
    "                 losses=losses,\n",
    "                 )\n",
    "print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f1f1ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6ffa1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6cd8316",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load your custom NER model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour_custom_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define some example sentences and their correct entity annotations\u001b[39;00m\n\u001b[1;32m      5\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI like apples and bananas\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMary lives in Paris\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "# Load your custom NER model\n",
    "nlp = spacy.load('your_custom_model')\n",
    "\n",
    "# Define some example sentences and their correct entity annotations\n",
    "sentences = [\n",
    "    \"I like apples and bananas\",\n",
    "    \"Mary lives in Paris\"\n",
    "]\n",
    "annotations = [\n",
    "    {\"entities\": [(7, 13, \"FRUIT\"), (18, 25, \"FRUIT\")]},\n",
    "    {\"entities\": [(0, 4, \"PERSON\"), (12, 17, \"GPE\")]}\n",
    "]\n",
    "\n",
    "# Calculate the accuracy of your model\n",
    "scorer = Scorer()\n",
    "for i in range(len(sentences)):\n",
    "    doc_gold = nlp.make_doc(sentences[i])\n",
    "    gold = GoldParse(doc_gold, entities=annotations[i]['entities'])\n",
    "    doc_pred = nlp(sentences[i])\n",
    "    scorer.score(doc_pred, gold)\n",
    "\n",
    "accuracy = scorer.scores['ents_f']\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb6f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@5.249] global loadsave.cpp:244 findDecoder imread_('C://Desktop/Python/Projects/OCR Invoice Detection/Images/Blank Invoice.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter name of the image: /home/sunil/Desktop/np/data_local/large.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@74.634] global loadsave.cpp:244 findDecoder imread_('C://Desktop/Python/Projects/OCR Invoice Detection/Images//home/sunil/Desktop/np/data_local/large.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m scanned_copy, scanned_mask, scanned_image\n\u001b[0;32m---> 87\u001b[0m copy, mask, ogimage \u001b[38;5;241m=\u001b[39m \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m myfeatures \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m, in \u001b[0;36mresize\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m picture \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter name of the image: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(imagepath \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpicture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m w, h, _ \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m     49\u001b[0m dem \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     50\u001b[0m dem \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(dem,(\u001b[38;5;241m650\u001b[39m, \u001b[38;5;241m1000\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import cv2\n",
    "import pytesseract \n",
    "import numpy as np\n",
    "\n",
    "Query = cv2.imread(\"C://Desktop/Python/Projects/OCR Invoice Detection/Images/Blank Invoice.jpg\")\n",
    "# TestImage = cv2.imread(\"C://Desktop/Python/Projects/OCR Invoice Detection/Images/David.jpg\")\n",
    "\n",
    "# # Viewing the Query Image\n",
    "# h, w, c = Query.shape\n",
    "# cv2.imshow(\"Blank Invoice\", Query)\n",
    "# k = cv2.waitKey(0) & 0xFF\n",
    "# if k == 27:         # wait for ESC key to exit\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "orb = cv2.ORB_create(5000) # Increase this number if all features are not being detected \n",
    "tesseract_path = \"C:\\\\Python\\\\Anaconda3\\\\pkgs\\\\tesseract-4.1.1-had67df9_4\\\\Library\\\\bin\\\\tesseract.exe\"\n",
    "pytesseract.pytesseract.tesseract_cmd = tesseract_path\n",
    "keypoints, descriptors = orb.detectAndCompute(Query, None)\n",
    "\n",
    "# # Viewing the highlighted features \n",
    "# img_kp = cv2.drawKeypoints(TestImage, keypoints, None)\n",
    "# cv2.imshow(\"Blank Invoice with key points highlighted\", img_kp)\n",
    "# k = cv2.waitKey(0) & 0xFF\n",
    "# if k == 27:         # wait for ESC key to exit\n",
    "#     cv2.destroyAllWindows()\n",
    "\n",
    "# Initializing region of Interests\n",
    "\n",
    "Features = [[(369, 123), (875, 209), 'Customer Name'],   # Single Boxes\n",
    "            [(369, 212), (879, 256), 'Address'], \n",
    "            [(369, 318), (874, 363), 'Phone Number'],  \n",
    "            [(1292, 208), (1508, 260), 'Date'],   \n",
    "            [(74, 538), (864, 604), 'Description'],\n",
    "            [(1042, 555), (1296, 1198), 'Unit Price'], \n",
    "            [(1298, 540), (1508, 602), 'Amount'],\n",
    "            [(1301, 1201), (1515, 1246), 'Sub Total'],\n",
    "            [(1303, 1253), (1516, 1301), 'Tax Rate'], \n",
    "            [(1304, 1304), (1519, 1351), 'Tax'], \n",
    "            [(1304, 1354), (1520, 1400), 'Total Amount']]\n",
    "\n",
    "# Defining a function that resizes an input image using cv2.warpPerspective() and crops areas of interests\n",
    "def resize():\n",
    "    imagepath = \"C://Desktop/Python/Projects/OCR Invoice Detection/Images\"\n",
    "    picture = input(\"Enter name of the image: \")\n",
    "    image = cv2.imread(imagepath + f\"/{picture}\")\n",
    "    w, h, _ = image.shape\n",
    "    \n",
    "    dem = image.copy()\n",
    "    dem = cv2.resize(dem,(650, 1000))\n",
    "    cv2.imshow(\"Original Unwarped Image\", dem)\n",
    "    k = cv2.waitKey(0) & 0xFF\n",
    "    if k == 27:         # wait for ESC key to exit\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    keypoints_2, descriptors_2 = orb.detectAndCompute(image, None)\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "    matches = matcher.match(descriptors_2, descriptors)\n",
    "    matches = sorted(matches, key = lambda x:x.distance) \n",
    "    \n",
    "    best_matches = matches[:int(len(matches) * 0.25)] # 25% of the best matches\n",
    "    \n",
    "    # # Visualizing the matches\n",
    "    # pic_match = cv2.drawMatches(Query, keypoints, image, keypoints_2, best_matches[:30], None, flags = 2) \n",
    "    # pic_match = cv2.resize(pic_match, (w // 2, h // 2))\n",
    "    # cv2.imshow(\"Matching Features Detected\", pic_match)\n",
    "    # k = cv2.waitKey(0) & 0xFF\n",
    "    # if k == 27:         # Press ESC key to exit\n",
    "    #     cv2.destroyAllWindows()\n",
    "    \n",
    "    source_points = np.float32([keypoints_2[i.queryIdx].pt for i in best_matches]).reshape(-1, 1, 2)\n",
    "    destination_points = np.float32([keypoints[i.trainIdx].pt for i in best_matches]).reshape(-1, 1, 2)\n",
    "    \n",
    "    homography, _ = cv2.findHomography(source_points, destination_points, cv2.RANSAC, 5.0)\n",
    "    scanned_image = cv2.warpPerspective(image, homography, (Query.shape[1], Query.shape[0]))\n",
    "\n",
    "    scanned_copy = scanned_image.copy()\n",
    "    scanned_mask = np.zeros_like(scanned_copy)\n",
    "    demo = scanned_image.copy()\n",
    "    demo = cv2.resize(demo,(Query.shape[1] // 2, Query.shape[0] // 2))\n",
    "    cv2.imshow(\"Warped Image\", demo)\n",
    "    k = cv2.waitKey(0) & 0xFF\n",
    "    if k == 27:         # Press ESC key to exit\n",
    "        cv2.destroyAllWindows()\n",
    "        return scanned_copy, scanned_mask, scanned_image\n",
    "        \n",
    "copy, mask, ogimage = resize()\n",
    "\n",
    "myfeatures = []\n",
    "print()\n",
    "print(\"Pytesseract detected the following features:\")\n",
    "print()\n",
    "\n",
    "for i, feature in enumerate(Features):\n",
    "    \n",
    "    # # Drawing detected features\n",
    "    # cv2.rectangle(mask, (feature[0][0], feature[0][1]), (feature[1][0], feature[1][1]), (0, 255, 255), cv2.FILLED)\n",
    "    # copy = cv2.addWeighted(copy, 0.99, mask, 0.1, 0)\n",
    "    \n",
    "    crop = ogimage[feature[0][1] : feature[1][1], feature[0][0] : feature[1][0]] # Extracting features to feed into pytesseract  \n",
    "    \n",
    "    # # Viewing the crops\n",
    "    # cv2.imshow(\"Detected Features\", crop)\n",
    "    # k = cv2.waitKey(0) & 0xFF\n",
    "    # if k == 27:         # wait for ESC key to exit\n",
    "    #     cv2.destroyAllWindows()\n",
    "    \n",
    "    print(\"{}: {}\".format(str(feature[2]), re.sub(r'[^A-Za-z0-9/%]+', ' ', pytesseract.image_to_string(crop))))\n",
    "   \n",
    "    myfeatures.append(pytesseract.image_to_string(crop))\n",
    "    myfeatures_clean = [re.sub(r'[^A-Za-z0-9,/%]+', ' ', x) for x in myfeatures]\n",
    "    cv2.putText(copy, str(myfeatures_clean[i]), (feature[0][0], feature[0][1] - 20),\n",
    "                    cv2.FONT_HERSHEY_PLAIN, 3.5, (255, 50, 255), 3)\n",
    "\n",
    "w, h, _ = copy.shape\n",
    "copy = cv2.resize(copy, (w // 2, h // 2))\n",
    "cv2.imshow(\"Detected Features\", copy)\n",
    "k = cv2.waitKey(0) & 0xFF\n",
    "if k == 27:         # wait for ESC key to exit\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa190441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-15 10:21:45.884260: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v2 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59eaccaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'VERSION'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVERSION\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'VERSION'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09d1d5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;43mWARNING:\u001b[0minvoice2data.extract.loader:\u001b[1;43m json Loader Failed to load 1616564744invJay International - EInvoice.jpg template:\n",
      "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\u001b[0m\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'tpl' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minvoice2data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extract_data\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minvoice2data\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextract\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_templates\n\u001b[0;32m----> 4\u001b[0m templates \u001b[38;5;241m=\u001b[39m \u001b[43mread_templates\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/sunil/Desktop/np/data_local\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m result \u001b[38;5;241m=\u001b[39m extract_data(filename, templates\u001b[38;5;241m=\u001b[39mtemplates)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/invoice2data/extract/loader.py:80\u001b[0m, in \u001b[0;36mread_templates\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     79\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson Loader Failed to load \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m template:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name, error)\n\u001b[0;32m---> 80\u001b[0m \u001b[43mtpl\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemplate_name\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Test if all required fields are in template\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tpl\u001b[38;5;241m.\u001b[39mkeys():\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'tpl' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from invoice2data import extract_data\n",
    "from invoice2data.extract.loader import read_templates\n",
    "\n",
    "templates = read_templates('/home/sunil/Desktop/np/data_local')\n",
    "filename = 'r.json'\n",
    "result = extract_data(filename, templates=templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "693194b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mINFO:\u001b[0minvoice2data.extract.loader:\u001b[94m Loaded 172 templates from /home/sunil/.local/lib/python3.11/site-packages/invoice2data/extract/templates\u001b[0m\r\n",
      "\u001b[1;41mERROR:\u001b[0mroot:\u001b[1;41m No template for /home/sunil/Desktop/np/BILL.pdf\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!invoice2data --input-reader pdftotext /home/sunil/Desktop/np/BILL.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f25dbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;41mERROR:\u001b[0mroot:\u001b[1;41m No template for /home/sunil/Desktop/np/data_local/BILL.pdf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from invoice2data import extract_data\n",
    "result = extract_data('/home/sunil/Desktop/np/data_local/BILL.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbefb141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68cb6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
