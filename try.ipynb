{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24351743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d2f69b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2650708797.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 22\u001b[0;36m\u001b[0m\n\u001b[0;31m    invoiceNumber=,\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# invoiceNumber\tInvoiceDate\tDueDate\tProduct line\tUnit price\tQuantity\tTax 5%\tTotal\tDate\tTime\tPayment\tTELEFON\n",
    "categories = [\n",
    "    \"invoiceNumber\",\n",
    "    \"InvoiceDate\",\n",
    "    \"DueDate\",\n",
    "    \"Product line\",\n",
    "    \"Unit price\",\n",
    "    \"Tax 5%\",\n",
    "    \"Total\",\n",
    "    \"Date\",\n",
    "    \"Time\",\n",
    "    \"Payment\",\n",
    "    \"TELEFONE\"\n",
    "]\n",
    "\n",
    "news_group_data = text(\n",
    "    subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    dict(\n",
    "        invoiceNumber=,\n",
    "        InvoiceDate,\n",
    "        DueDate,\n",
    "        Product line,\n",
    "        Unit price,\n",
    "        Tax 5%,\n",
    "        Total,\n",
    "        Date,\n",
    "        Time,\n",
    "        Payment,\n",
    "        TELEFONE\n",
    "        text=news_group_data[\"data\"],\n",
    "        target=news_group_data[\"target\"]\n",
    "    )\n",
    ")\n",
    "df[\"target\"] = df.target.map(lambda x: categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_ocr\n",
    "\n",
    "pipeline = keras_ocr.pipeline.Pipeline()\n",
    "image = keras_ocr.tools.read('/home/sunil/Desktop/np/uploads/car-service.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675f9f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_groups = pipeline.recognize([image])\n",
    "texts = [word[0] for prediction in prediction_groups for word in prediction]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ee894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "model = tf.keras.models.load_model('text_classification_model.h5')\n",
    "classes = ['invoiceNumber','InvoiceDate','DueDate','Product line','Unit price','Quantity','Tax 5%','Total','Date','Time','Payment','TELEFONE']  # replace with your own class labels\n",
    "\n",
    "for text in texts:\n",
    "    # preprocess the text\n",
    "    processed_text = preprocess(text)\n",
    "\n",
    "    # predict the class\n",
    "    prediction = model.predict(processed_text)\n",
    "    predicted_class = classes[prediction.argmax()]\n",
    "    \n",
    "    print(f'Text: {text} - Predicted class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2661d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from langdetect import detect\n",
    "import cv2\n",
    "from wand.image import Image as wi   \n",
    "from pdf2image import convert_from_path\n",
    "import io      \n",
    "import matplotlib.pyplot as plt\n",
    "# import keras_ocr\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dbb89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = \"/home/sunil/Desktop/np/data_local/\"\n",
    "for root, dirs, filenames in os.walk(indir):\n",
    "    for filename in filenames:\n",
    "        print('#######' + filename + '#######')\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "#             gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#             thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "            im = Image.open(os.path.join(indir, filename))\n",
    "            text = pytesseract.image_to_string(im, lang='eng')\n",
    "            print(text)\n",
    "        elif filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(indir, filename)\n",
    "            images = convert_from_path(pdf_path)\n",
    "            for i, image in enumerate(images):\n",
    "                image.save(f'uploads/page_{i+1}.jpg', 'JPEG')\n",
    "        else:\n",
    "            print(\"Input the correct file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8126ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Restaurant Receipt\n",
    "\n",
    "Name Jane Smith 002\n",
    "\n",
    "Receipt #\n",
    "\n",
    "Email jasmith@noemail.com  Date 4/8/2019\n",
    "\n",
    "Phone Number (123) 123-4567 Time 5:00 PM\n",
    "\n",
    "Ordered Items\n",
    "\n",
    "Food Name Quantity Price Total\n",
    "\n",
    "1 Food A 5 5 25\n",
    "\n",
    "2 Food B 5 5 25\n",
    "\n",
    "3 Food C 5 5 25\n",
    "\n",
    "4 Drink D 5 2 10\n",
    "\n",
    "5 Drink E 5 2 10\n",
    "Subtotal $95\n",
    "Tax % 5\n",
    "\n",
    "I\n",
    "\n",
    "Total $99.75\n",
    "\n",
    "Hope you enjoy your dining experience!\n",
    "\n",
    "1929 Brown Bear Drive, Elsinore,CA, 92330\n",
    "Phone: (123) 1234567 - nfo@xyzrestaurant com - st xyzrestaurant com\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d63bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "maxlen = 100  # maximum length of input sequence\n",
    "num_words = 10000  # maximum number of words in the tokenizer's vocabulary\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "X = tokenizer.texts_to_sequences(text)\n",
    "X = pad_sequences(X, maxlen=maxlen, padding='post')\n",
    "\n",
    "y = ['invoiceNumber','InvoiceDate','DueDate','Product line','Unit price','Quantity','Tax 5%','Total','Date','Time','Payment','TELEFONE']  # replace with your own label data\n",
    "\n",
    "# split the dataset into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3b6434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d6fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338060be",
   "metadata": {},
   "outputs": [],
   "source": [
    "<wordcloud.wordcloud.WordCloud at 0x7faf5a245ba8>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7623b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ./prodigy.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"model-best\")  # load the spacy model\n",
    "\n",
    "def process_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    return pytesseract.image_to_string(thresh, lang='eng')\n",
    "\n",
    "def process_file(file_path):\n",
    "    text = \"\"\n",
    "    if file_path.endswith(\".jpg\") or file_path.endswith(\".jpeg\") or file_path.endswith(\".png\"):\n",
    "        im = cv2.imread(file_path)\n",
    "        text = process_image(im)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        images = convert_from_path(file_path)\n",
    "        for i, image in enumerate(images):\n",
    "            image.save(f'special/page_{i+1}.jpg', 'JPEG')\n",
    "            text += process_image(image)\n",
    "    else:\n",
    "        print(\"Input the correct file\")\n",
    "    return text\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    for root, dirs, filenames in os.walk(directory_path):\n",
    "        for filename in filenames:\n",
    "#             print('#######' + filename + '#######')\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            text = process_file(file_path)\n",
    "            doc = nlp(text)\n",
    "            for ent in doc.ents:\n",
    "                print(ent.text, ent.label_,)\n",
    "\n",
    "process_directory(\"/home/sunil/Desktop/np/uploads/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ce314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "import spacy\n",
    "import shutil\n",
    "\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\")  # load the spacy model\n",
    "\n",
    "def process_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    return pytesseract.image_to_string(thresh, lang='eng')\n",
    "\n",
    "def process_file(file_path):\n",
    "    text = \"\"\n",
    "    if file_path.endswith(\".jpg\") or file_path.endswith(\".jpeg\") or file_path.endswith(\".png\"):\n",
    "        im = cv2.imread(file_path)\n",
    "        text = process_image(im)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        images = convert_from_path(file_path)\n",
    "        for i, image in enumerate(images):\n",
    "            image.save(f'special/page_{i+1}.jpg', 'JPEG')\n",
    "            text += process_image(image)\n",
    "    else:\n",
    "        print(\"Input the correct file\")\n",
    "    return text\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    for root, dirs, filenames in os.walk(input_dir):\n",
    "        for filename in filenames:\n",
    "            print('#######' + filename + '#######')\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            text = process_file(input_path)\n",
    "            doc = nlp(text)\n",
    "            for ent in doc.ents:\n",
    "                print(ent.text, ent.label_)\n",
    "            output_path = os.path.join(output_dir, filename)\n",
    "            shutil.move(input_path, output_path)\n",
    "\n",
    "process_directory(\"/home/sunil/Desktop/np/uploads\", \"/home/sunil/Desktop/np/special\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae57006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "import cv2\n",
    "import pytesseract\n",
    "import spacy\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ae5205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\")  # load the spacy model\n",
    "\n",
    "def process_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    return pytesseract.image_to_string(thresh, lang='eng')\n",
    "\n",
    "def process_file(file_path):\n",
    "    text = \"\"\n",
    "    if file_path.endswith(\".jpg\") or file_path.endswith(\".jpeg\") or file_path.endswith(\".png\"):\n",
    "        im = cv2.imread(file_path)\n",
    "        text = process_image(im)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        images = convert_from_path(file_path)\n",
    "        for i, image in enumerate(images):\n",
    "            image.save(f'special/page_{i+1}.jpg', 'JPEG')\n",
    "            text += process_image(image)\n",
    "    else:\n",
    "        print(\"Input the correct file\")\n",
    "    return text\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    rows = []\n",
    "    for root, dirs, filenames in os.walk(input_dir):\n",
    "        for filename in filenames:\n",
    "            print('#######' + filename + '#######')\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            text = process_file(input_path)\n",
    "            print(\"Processed text:\")\n",
    "#             print(text)\n",
    "            doc = nlp(text)\n",
    "            mylist = []\n",
    "            for ent in doc.ents:\n",
    "                print(ent.text, ent.label_)\n",
    "                mylist.append([ent.text,ent.label_])\n",
    "            type_bill = ','.join(i[0] for i in mylist if i[1] == 'TYPE_OF_BILL')\n",
    "            name = ','.join(i[0] for i in mylist if i[1] == 'NAME')\n",
    "            invoice = ','.join(i[0] for i in mylist if i[1] == 'INVOICE_NO./BILL_NO.')\n",
    "            email = ','.join(i[0] for i in mylist if i[1] == 'EMAIL')\n",
    "            date = ','.join(i[0] for i in mylist if i[1] == 'ORDER_DATE')\n",
    "            description = ','.join(i[0] for i in mylist if i[1] == 'DESCRIPTION')\n",
    "            amount = ','.join(i[0] for i in mylist if i[1] == 'TOTAL_AMOUNT')\n",
    "            tax = ','.join(i[0] for i in mylist if i[1] == 'TAX_RATE')\n",
    "            quantity = ','.join(i[0] for i in mylist if i[1] == 'QUANTITY')\n",
    "            mobile = ','.join(i[0] for i in mylist if i[1] == 'MOBILE_No.')\n",
    "            df = pd.DataFrame(columns=['TYPE_OF_BILL', 'NAME', 'INVOICE_NO./BILL_NO.', 'EMAIL', 'ORDER_DATE', 'DESCRIPTION', 'TOTAL_AMOUNT', 'TAX_RATE', 'QUANTITY', 'MOBILE_NO.'])\n",
    "            df = df.append({'TYPE_OF_BILL':type_bill,'NAME':name,'INVOICE_NO./BILL_NO.':invoice,'EMAIL':email,'ORDER_DATE':date,'DESCRIPTION':description,'TOTAL_AMOUNT':amount,'TAX_RATE':tax,'QUANTITY':quantity,'MOBILE_NO.':mobile}, ignore_index=True)\n",
    "            print(df)\n",
    "            html_table = df.to_html()\n",
    "\n",
    "            # write the HTML table to a file\n",
    "            with open('output.html', 'w') as f:\n",
    "                f.write(html_table)\n",
    "            output_path = \"output.html\"\n",
    "            shutil.move(input_path, output_path)\n",
    "            print(f\"Moved file to {output_path}\")\n",
    "            \n",
    "            \n",
    "process_directory(r\"/home/sunil/Desktop/np/uploads/\", r\"/home/sunil/Desktop/np/special/\")\n",
    "#     if not rows:\n",
    "#         print(\"No entities found\")\n",
    "#     else:\n",
    "#         df = pd.DataFrame(rows, columns=['TYPE_OF_BILL','EMAIL','ORDER_DATE','MOBILE_NO.','TIME','DESCRIPTION','TAX_RATE','TOTAL_AMOUNT','INVOICE_NO./BILL_NO.','STATE','QUANTITY'])\n",
    "#         print(df)\n",
    "#         return df\n",
    "\n",
    "# print(df)\n",
    "# def data_column():\n",
    "#     df = pd.DataFrame(ent.text, columns=['TYPE_OF_BILL','EMAIL','ORDER_DATE','MOBILE_NO.','TIME','DESCRIPTION','TAX_RATE','TOTAL_AMOUNT','INVOICE_NO./BILL_NO.','STATE','QUANTITY'])\n",
    "#     return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf928db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('model-best') \n",
    "sentence = \"\"\"g Service Receipt\n",
    "\n",
    "04/13/2020\n",
    "AUTOTECH SERVICE CENTER - 8980 West Lake Pond Dr. Long Beach, CA 90813\n",
    "Bill To\n",
    "Michael Carlsmilth\n",
    "(958) 2035829\n",
    "michael@example.com\n",
    "123 Rodeo Drive, 213 Mark Street\n",
    "Great City, Some State, 1111\n",
    "United States.\n",
    "Car Make Car Model YearModel  Color Transmission Engine Type\n",
    "BMW 3161 1999 Red  Automatic Gas\n",
    "Last Change Oil Mileage Reading Next Change Ol\n",
    "Apr 13,2020 99000 04/13/2021\n",
    "Under Chassis Hood\n",
    "Tie Rod Ends, Idler Arm Belts, Spark plugs, Oil, ATF\n",
    "Brakes Bearing and Shocks\n",
    "Wheel Cylinder\n",
    "Materials Labor\n",
    "Particulars ‘Amount Particulars ‘Amount\n",
    "Oil Filter 20 Change Oil 50\n",
    "Synthetic Ol 200 belt replacement 100\n",
    "Spark plugs 100\n",
    "\n",
    "Total Labor Cost $150\n",
    "\n",
    "Total Material Cost  $320\n",
    "\n",
    "Total Cost $470\"\"\"\n",
    "doc = nlp(sentence) \n",
    "\n",
    "mylist= []\n",
    "for ent in doc.ents: \n",
    "    print(ent.text, ent.label_)\n",
    "    mylist.append([ent.text,ent.label_])\n",
    "\n",
    "# print(mylist)\n",
    "\n",
    "type_bill = ','.join(i[0] for i in mylist if i[1] == 'TYPE_OF_BILL')\n",
    "name = ','.join(i[0] for i in mylist if i[1] == 'NAME')\n",
    "invoice = ','.join(i[0] for i in mylist if i[1] == 'INVOICE_NO./BILL_NO.')\n",
    "email = ','.join(i[0] for i in mylist if i[1] == 'EMAIL')\n",
    "date = ','.join(i[0] for i in mylist if i[1] == 'ORDER_DATE')\n",
    "description = ','.join(i[0] for i in mylist if i[1] == 'DESCRIPTION')\n",
    "amount = ','.join(i[0] for i in mylist if i[1] == 'TOTAL_AMOUNT')\n",
    "tax = ','.join(i[0] for i in mylist if i[1] == 'TAX_RATE')\n",
    "quantity = ','.join(i[0] for i in mylist if i[1] == 'QUANTITY')\n",
    "mobile = ','.join(i[0] for i in mylist if i[1] == 'MOBILE_No.')\n",
    "\n",
    "df = df.append({'TYPE_OF_BILL':type_bill,'NAME':name,'INVOICE_NO./BILL_NO.':invoice,'EMAIL':email,'ORDER_DATE':date,'DESCRIPTION':description,'TOTAL_AMOUNT':amount,'TAX_RATE':tax,'QUANTITY':quantity,'MOBILE_NO.':mobile}, ignore_index=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f675de",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = df.to_html()\n",
    "\n",
    "#write html to file\n",
    "text_file = open(\"output.html\", \"w\")\n",
    "text_file.write(html)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3698ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\")  # load the spacy model\n",
    "\n",
    "def process_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    return pytesseract.image_to_string(thresh, lang='eng')\n",
    "\n",
    "def process_file(file_path):\n",
    "    text = \"\"\n",
    "    if file_path.endswith(\".jpg\") or file_path.endswith(\".jpeg\") or file_path.endswith(\".png\"):\n",
    "        im = cv2.imread(file_path)\n",
    "        text = process_image(im)\n",
    "    elif file_path.endswith(\".pdf\"):\n",
    "        images = convert_from_path(file_path)\n",
    "        for i, image in enumerate(images):\n",
    "            image.save(f'special/page_{i+1}.jpg', 'JPEG')\n",
    "            text += process_image(image)\n",
    "    else:\n",
    "        print(\"Input the correct file\")\n",
    "    return text\n",
    "\n",
    "def process_directory(input_dir, output_dir):\n",
    "    rows = []\n",
    "    df = pd.DataFrame(columns=['TYPE_OF_BILL', 'NAME', 'INVOICE_NO./BILL_NO.', 'EMAIL', 'ORDER_DATE', 'DESCRIPTION', 'TOTAL_AMOUNT', 'TAX_RATE', 'QUANTITY', 'MOBILE_NO.'])\n",
    "    for root, dirs, filenames in os.walk(input_dir):\n",
    "        for filename in filenames:\n",
    "            print('#######' + filename + '#######')\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            text = process_file(input_path)\n",
    "            print(\"Processed text:\")\n",
    "#             print(text)\n",
    "            doc = nlp(text)\n",
    "            mylist = []\n",
    "            for ent in doc.ents:\n",
    "                print(ent.text, ent.label_)\n",
    "                mylist.append([ent.text,ent.label_])\n",
    "            print(type(mylist))\n",
    "            type_bill = ','.join(i[0] for i in mylist if i[1] == 'TYPE_OF_BILL')\n",
    "            name = ','.join(i[0] for i in mylist if i[1] == 'NAME')\n",
    "            invoice = ','.join(i[0] for i in mylist if i[1] == 'INVOICE_NO./BILL_NO.')\n",
    "            email = ','.join(i[0] for i in mylist if i[1] == 'EMAIL')\n",
    "            date = ','.join(i[0] for i in mylist if i[1] == 'ORDER_DATE')\n",
    "            description = ','.join(i[0] for i in mylist if i[1] == 'DESCRIPTION')\n",
    "            amount = ','.join(i[0] for i in mylist if i[1] == 'TOTAL_AMOUNT')\n",
    "            tax = ','.join(i[0] for i in mylist if i[1] == 'TAX_RATE')\n",
    "            quantity = ','.join(i[0] for i in mylist if i[1] == 'QUANTITY')\n",
    "            mobile = ','.join(i[0] for i in mylist if i[1] == 'MOBILE_No.')\n",
    "            df = df.append({'TYPE_OF_BILL':type_bill,'NAME':name,'INVOICE_NO./BILL_NO.':invoice,'EMAIL':email,'ORDER_DATE':date,'DESCRIPTION':description,'TOTAL_AMOUNT':amount,'TAX_RATE':tax,'QUANTITY':quantity,'MOBILE_NO.':mobile}, ignore_index=True)\n",
    "            print(type(df))\n",
    "            output_path = \"output.html\"\n",
    "            shutil.move(input_path, output_path)\n",
    "            print(f\"Moved file to {output_path}\")\n",
    "    return df\n",
    "process_directory(r\"/home/sunil/Desktop/np/uploads/\", r\"/home/sunil/Desktop/np/special/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4128b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# Define output folder to save new model\n",
    "# model_dir = 'D:/Anindya/E/model'\n",
    "model_dir = '/home/sunil/Desktop/np/annotations.json'\n",
    "# Train new NER model\n",
    "def train_new_NER(model=None, output_dir=model_dir, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "            print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "            \n",
    "# Finally train the model by calling above function\n",
    "train_new_NER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3004dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model_dir = 'D:/Anindya/E/updated_model'\n",
    "\n",
    "## Update existing spacy model and store into a folder\n",
    "def update_model(model='en_core_web_sm', output_dir=updated_model_dir, n_iter=100):\n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])\n",
    "        print(\"Tokens\", [(t.text, t.ent_type_, t.ent_iob) for t in doc])\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Finally train the model by calling above function        \n",
    "update_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    # return pytesseract.image_to_string(thresh, lang='eng')\n",
    "    image_text=pytesseract.image_to_string(thresh, lang='eng')\n",
    "    return image_text \n",
    "def get_data():\n",
    "    alpha = process_image()\n",
    "    print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27459143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89338787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\") \n",
    "alha = \"\"\"Restaurant Receipt\n",
    "\n",
    "Name Jane Smith 002\n",
    "\n",
    "Receipt #\n",
    "\n",
    "Email jasmith@noemail.com  Date 4/8/2019\n",
    "\n",
    "Phone Number (123) 123-4567 Time 5:00 PM\n",
    "\n",
    "Ordered Items\n",
    "\n",
    "Food Name Quantity Price Total\n",
    "\n",
    "1 Food A 5 5 25\n",
    "\n",
    "2 Food B 5 5 25\n",
    "\n",
    "3 Food C 5 5 25\n",
    "\n",
    "4 Drink D 5 2 10\n",
    "\n",
    "5 Drink E 5 2 10\n",
    "Subtotal $95\n",
    "Tax % 5\n",
    "\n",
    "I\n",
    "\n",
    "Total $99.75\n",
    "\n",
    "Hope you enjoy your dining experience!\n",
    "\n",
    "\"\"\"\n",
    "# filtered_entities = {}\n",
    "# for ent in doc.ents:\n",
    "#     if ent.label_ in [\"TYPE_OF_BILL\",\"DATE\",\"MOBILE_NO\",\"ADDRESS\",\"DESCRIPTION\",\"STATE\",\"INVOICE/BILL_NO\",\"EMAIL\",\"TIME\",\"AMOUNT\",\"RATE\",\"QUANTITY\",\"NAME\",\"COUNTRY\"]:\n",
    "#         if ent.label_ not in filtered_entities:\n",
    "#             filtered_entities[ent.label_] = []\n",
    "#         filtered_entities[ent.label_].append(ent.text)\n",
    "\n",
    "# print(filtered_entities)\n",
    "# entities= []\n",
    "output = [\n",
    "    alpha,\n",
    "    {\n",
    "        \"entities\":[]\n",
    "    }\n",
    "    \n",
    "]\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ in [\"TYPE_OF_BILL\",\"DATE\",\"MOBILE_NO\",\"ADDRESS\",\"DESCRIPTION\",\"STATE\",\"INVOICE/BILL_NO\",\"EMAIL\",\"TIME\",\"AMOUNT\",\"RATE\",\"QUANTITY\",\"NAME\",\"COUNTRY\"] :\n",
    "        output[1]['entities'].append([ent.start_char, ent.end_char, ent.label_])\n",
    "    \n",
    "print(output)\n",
    "# to get the data in required json format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('/home/sunil/Desktop/np/annotations.json', 'r') as f:\n",
    "# #     TEST_DATA = json.load(f)\n",
    "#     TRAIN_DATA = json.load(f)#         file_data[\"annotations\"][0].append(new_data)\n",
    "def write_json(new_data,filename='annotations.json'):\n",
    "    with open(filename, 'r+') as file:\n",
    "        file_data = json.load(file)\n",
    "        file_data.update({\"annotations\":[new_data]})\n",
    "        file.seek(1)\n",
    "        json.dump(file_data,file,indent=4)\n",
    "    \n",
    "        \n",
    "y = output\n",
    "write_json(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db338c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_json(new_data, filename='data.json'):\n",
    "#     with open(filename,'r+') as file:\n",
    "#           # First we load existing data into a dict.\n",
    "#         file_data = json.load(file)\n",
    "#         # Join new_data with file_data inside emp_details\n",
    "#         file_data[\"emp_details\"].append(new_data)\n",
    "#         # Sets file's current position at offset.\n",
    "#         file.seek(0)\n",
    "#         # convert back to json.\n",
    "#         json.dump(file_data, file, indent = 4)\n",
    " \n",
    "#     # python object to be appended\n",
    "# y = {\"emp_name\":\"Nikhil\",\n",
    "#      \"email\": \"nikhil@geeksforgeeks.org\",\n",
    "#      \"job_profile\": \"Full Time\"\n",
    "#     }\n",
    "     \n",
    "# write_json(y)\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08446aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# load the existing NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# load the training data\n",
    "train_data = load_train_data()\n",
    "\n",
    "# create new Examples from the training data\n",
    "examples = []\n",
    "for text, annotations in train_data:\n",
    "    example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "    examples.append(example)\n",
    "\n",
    "# train the NER model with the new data\n",
    "nlp.disable_pipes(\"tagger\", \"parser\")\n",
    "optimizer = nlp.begin_training()\n",
    "for i in range(10):\n",
    "    losses = {}\n",
    "    batches = minibatch(examples, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        nlp.update(batch, sgd=optimizer, losses=losses)\n",
    "    print(\"Losses\", losses)\n",
    "\n",
    "# save the updated model\n",
    "nlp.to_disk(\"updated_ner_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc55f7",
   "metadata": {},
   "source": [
    "\n",
    "# REINFORCEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained spaCy NER model\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\")\n",
    "\n",
    "# Define the labels for named entities\n",
    "LABELS = [\"TYPE_OF_BILL\",\"DATE\",\"MOBILE_NO\",\"ADDRESS\",\"DESCRIPTION\",\"STATE\",\"INVOICE/BILL_NO\",\"EMAIL\",\"TIME\",\"AMOUNT\",\"RATE\",\"QUANTITY\",\"NAME\",\"COUNTRY\"]\n",
    "\n",
    "# Define the Q-table\n",
    "q_table = np.zeros((len(LABELS), len(LABELS)))\n",
    "\n",
    "# Define the learning rate and discount factor\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "# Define a function to extract features from a sentence\n",
    "def extract_features(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    features = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in LABELS:\n",
    "            features.append(token.ent_type_)\n",
    "    return features\n",
    "\n",
    "# Define a function to choose an action based on the Q-table and exploration rate\n",
    "def choose_action(state, exploration_rate):\n",
    "    if random.uniform(0, 1) < exploration_rate:\n",
    "        return random.randint(0, len(LABELS) - 1)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# Define a function to update the Q-table based on a state, action, reward, and next state\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    old_value = q_table[state][action]\n",
    "    next_max = np.max(q_table[next_state])\n",
    "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "    q_table[state][action] = new_value\n",
    "\n",
    "# Train the model using reinforcement learning\n",
    "exploration_rate = 1.0\n",
    "for i in range(1000):\n",
    "    # Choose a sentence to train on\n",
    "    sentence = \"John works at Apple in California\"\n",
    "    features = extract_features(sentence)\n",
    "\n",
    "    # Choose an action based on the current state and exploration rate\n",
    "    state = features[0]\n",
    "    action = choose_action(state, exploration_rate)\n",
    "\n",
    "    # Apply the action and observe the next state and reward\n",
    "    next_state = LABELS[action]\n",
    "    if next_state in features:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = -1\n",
    "\n",
    "    # Update the Q-table based on the current state, action, reward, and next state\n",
    "    update_q_table(state, action, reward, next_state)\n",
    "\n",
    "    # Decay the exploration rate\n",
    "    exploration_rate *= 0.99\n",
    "\n",
    "# Use the Q-table to predict named entities in a new sentence\n",
    "sentence = \"\"\"Restaurant Receipt\n",
    "\n",
    "Name Jane Smith 002\n",
    "\n",
    "Receipt #\n",
    "\n",
    "Email jasmith@noemail.com  Date 4/8/2019\n",
    "\n",
    "Phone Number (123) 123-4567 Time 5:00 PM\n",
    "\n",
    "Ordered Items\n",
    "\n",
    "Food Name Quantity Price Total\n",
    "\n",
    "1 Food A 5 5 25\n",
    "\n",
    "2 Food B 5 5 25\n",
    "\n",
    "3 Food C 5 5 25\n",
    "\n",
    "4 Drink D 5 2 10\n",
    "\n",
    "5 Drink E 5 2 10\n",
    "Subtotal $95\n",
    "Tax % 5\n",
    "\n",
    "Total $99.75\n",
    "\"\"\"\n",
    "features = extract_features(sentence)\n",
    "state = features[0]\n",
    "action = np.argmax(q_table[state])\n",
    "prediction = LABELS[action]\n",
    "print(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c22515",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_index = LABELS.index(state)\n",
    "action = np.argmax(q_table[state_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f3834",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained spaCy NER model\n",
    "nlp = spacy.load(\"/home/sunil/Desktop/np/model-best\")\n",
    "\n",
    "# Define the labels for named entities\n",
    "LABELS = [\"TYPE_OF_BILL\",\"DATE\",\"MOBILE_NO\",\"ADDRESS\",\"DESCRIPTION\",\"STATE\",\"INVOICE/BILL_NO\",\"EMAIL\",\"TIME\",\"AMOUNT\",\"RATE\",\"QUANTITY\",\"NAME\",\"COUNTRY\"]\n",
    "\n",
    "# Define the Q-table\n",
    "q_table = np.zeros((len(LABELS), len(LABELS)))\n",
    "\n",
    "# Define the learning rate and discount factor\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "# Define a function to extract features from a sentence\n",
    "def extract_features(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    features = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in LABELS:\n",
    "            features.append(token.ent_type_)\n",
    "    return features\n",
    "\n",
    "# Define a function to choose an action based on the Q-table and exploration rate\n",
    "def choose_action(state, exploration_rate):\n",
    "    if random.uniform(0, 1) < exploration_rate:\n",
    "        return random.randint(0, len(LABELS) - 1)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# Define a function to update the Q-table based on a state, action, reward, and next state\n",
    "def update_q_table(state, action, reward, next_state):\n",
    "    old_value = q_table[state][action]\n",
    "    next_max = np.max(q_table[next_state])\n",
    "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "    q_table[state][action] = new_value\n",
    "\n",
    "# Train the model using reinforcement learning\n",
    "exploration_rate = 1.0\n",
    "for i in range(1000):\n",
    "    # Choose a sentence to train on\n",
    "    sentence = \"John works at Apple in California\"\n",
    "    features = extract_features(sentence)\n",
    "\n",
    "    # Choose an action based on the current state and exploration rate\n",
    "    state = features[0]\n",
    "    action = choose_action(state, exploration_rate)\n",
    "\n",
    "    # Apply the action and observe the next state and reward\n",
    "    next_state = LABELS[action]\n",
    "    if next_state in features:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = -1\n",
    "\n",
    "    # Update the Q-table based on the current state, action, reward, and next state\n",
    "    if len(features) > 0:\n",
    "        state_index = LABELS.index(state)\n",
    "        next_state_index = LABELS.index(next_state)\n",
    "        update_q_table(state_index, action, reward, next_state_index)\n",
    "\n",
    "    # Decay the exploration rate\n",
    "    exploration_rate *= 0.99\n",
    "\n",
    "# Use the Q-table to predict named entities in a new sentence\n",
    "sentence = \"\"\"Restaurant Receipt\n",
    "\n",
    "Name Jane Smith 002\n",
    "\n",
    "Receipt #\n",
    "\n",
    "Email jasmith@noemail.com  Date 4/8/2019\n",
    "\n",
    "Phone Number (123) 123-4567 Time 5:00 PM\n",
    "\n",
    "\n",
    "Ordered Items\n",
    "\n",
    "Food Name Quantity Price Total\n",
    "\n",
    "1 Food A 5 5 25\n",
    "\n",
    "2 Food B\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON data\n",
    "data = json.loads('{\"_id\": {\"$oid\": \"644a0f6cb75860c78f61e664\"},\"TYPE_OF_BILL\": \"\",\"DATE\": \"21-Dec-20,20-Dec-20\",\"MOBILE_NO\": \"\",\"ADDRESS\": \"Surabhi Hardwares, Bangalore,Surabhi Hardwares, Bangalore\",\"DESCRIPTION\": \"\",\"STATE\": \"Karnataka,Karnataka,Karnataka\",\"INVOICE/BILL_NO\": \"SHB/456/20\",\"EMAIL\": \"\",\"TIME\": \"\",\"AMOUNT\": \"3,500.00,4,130.00,3,500.00,315.00,315.00\",\"RATE\": \"315.00,315.00,9%,9%\",\"QUANTITY\": \"\",\"NAME\": \"Kiran Enterprises,Kiran Enterprises\",\"COUNTRY\": \"\"}')\n",
    "\n",
    "# Define the environment as the set of states\n",
    "states = [data]\n",
    "\n",
    "# Define the actions as buy or not buy\n",
    "actions = ['buy', 'not_buy']\n",
    "\n",
    "# Define the Q-table as a dictionary\n",
    "Q = {}\n",
    "for state in states:\n",
    "    for action in actions:\n",
    "        Q[str(state), action] = 0\n",
    "\n",
    "# Define the reward function as profit earned from buying the product\n",
    "def reward(state, action):\n",
    "    if action == 'buy':\n",
    "        amount = state['AMOUNT']\n",
    "        rate = state['RATE']\n",
    "        amount_list = [float(x.replace(',', '')) for x in amount.split(',')]\n",
    "        rate_list = [float(x.replace('%', ''))/100 if x.endswith('%') else float(x) for x in rate.split(',')]\n",
    "        total_cost = sum([a*r for a, r in zip(amount_list, rate_list)])\n",
    "        return total_cost\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Define the learning rate and discount factor\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "# Run the Q-learning algorithm for a fixed number of episodes\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    # Choose a random initial state\n",
    "    state = np.random.choice(states)\n",
    "    while True:\n",
    "        # Choose an action using an epsilon-greedy policy\n",
    "        epsilon = 0.1\n",
    "        if np.random.uniform() < epsilon:\n",
    "            action = np.random.choice(actions)\n",
    "        else:\n",
    "            q_values = [Q[str(state), a] for a in actions]\n",
    "            action = actions[np.argmax(q_values)]\n",
    "        \n",
    "        # Calculate the reward for the chosen action\n",
    "        r = reward(state, action)\n",
    "        \n",
    "        # Transition to the next state based on the chosen action\n",
    "        next_state = np.random.choice(states)\n",
    "        \n",
    "        # Update the Q-table using the Q-learning update rule\n",
    "        # Calculate the maximum Q-value for the next state and all possible actions\n",
    "        q_values_next = [Q[str(next_state), a] for a in actions]\n",
    "        max_q_next = np.max(q_values_next)\n",
    "        \n",
    "        # Update the Q-value for the current state-action pair\n",
    "        Q[str(state), action] += alpha * (r + gamma * max_q_next - Q[str(state), action])\n",
    "        \n",
    "        # Transition to the next state\n",
    "        state = next_state\n",
    "        \n",
    "        # Check if the episode has ended\n",
    "        if episode == num_episodes - 1:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cacf0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb+srv://aarika:ajain%40012023@cluster0.y932s1f.mongodb.net')\n",
    "db = client[\"OCR\"]\n",
    "collection = db[\"my_collection\"]\n",
    "data = pd.DataFrame(list(collection.find()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0946f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d54857",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9d56fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 14 (2702562700.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    path = '/home/sunil/Desktop/np/data_local/jot.pdf'\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 14\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfFileReader\n",
    "def get_info(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        pdf = PdfFileReader(f)\n",
    "        info = pdf.getDocumentInfo()\n",
    "        number_of_pages = pdf.getNumPages()\n",
    "        print(info)\n",
    "        author = info.author\n",
    "        creator = info.creator\n",
    "        producer = info.producer\n",
    "        subject = info.subject\n",
    "        title = info.title\n",
    "        return title\n",
    "if __name__ == '__main__':\n",
    "path = '/home/sunil/Desktop/np/data_local/jot.pdf'\n",
    "get_info(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0b4c81f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/home/sunil/Desktop/np/ocr_data/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpdfreader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PDFDocument, SimplePDFViewer\n\u001b[1;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/sunil/Desktop/np/ocr_data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     viewer \u001b[38;5;241m=\u001b[39m SimplePDFViewer(f)\n\u001b[1;32m      6\u001b[0m     viewer\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/home/sunil/Desktop/np/ocr_data/'"
     ]
    }
   ],
   "source": [
    "from pdfreader import PDFDocument, SimplePDFViewer\n",
    "file_path = r'/home/sunil/Desktop/np/ocr_data/'\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    viewer = SimplePDFViewer(f)\n",
    "    viewer.render()\n",
    "    text = \"\".join(viewer.canvas.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d662ee24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pdfreader\n",
      "  Using cached pdfreader-0.1.12-py3-none-any.whl\n",
      "Collecting bitarray>=1.1.0 (from pdfreader)\n",
      "  Using cached bitarray-2.7.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /home/sunil/.local/lib/python3.11/site-packages (from pdfreader) (9.4.0)\n",
      "Collecting pycryptodome>=3.9.9 (from pdfreader)\n",
      "  Using cached pycryptodome-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/sunil/.local/lib/python3.11/site-packages (from pdfreader) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pdfreader) (1.16.0)\n",
      "Installing collected packages: bitarray, pycryptodome, pdfreader\n",
      "Successfully installed bitarray-2.7.3 pdfreader-0.1.12 pycryptodome-3.17\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfreader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700570d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
